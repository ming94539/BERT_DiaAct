{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "# Sentiment Analysis with BERT\n",
    "\n",
    "> TL;DR In this tutorial, you'll learn how to fine-tune BERT for sentiment analysis. You'll do the required text preprocessing (special tokens, padding, and attention masks) and build a Sentiment Classifier using the amazing Transformers library by Hugging Face!\n",
    "\n",
    "- [Read the tutorial](https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
    "- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S)\n",
    "- [Read the `Getting Things Done with Pytorch` book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- Intuitively understand what BERT is\n",
    "- Preprocess text data for BERT and build PyTorch Dataset (tokenization, attention masks, and padding)\n",
    "- Use Transfer Learning to build Sentiment Classifier using the Transformers library by Hugging Face\n",
    "- Evaluate the model on test data\n",
    "- Predict sentiment on raw text\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "NJ6MhJYYBCwu",
    "outputId": "fc1bc767-990a-486d-bb10-2a80cea128b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 31 18:56:30 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M60           Off  | 00000001:00:00.0 Off |                  Off |\r\n",
      "| N/A   30C    P8    14W / 150W |     11MiB /  8129MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tbodro8Fpmwr"
   },
   "source": [
    "## What is BERT?\n",
    "\n",
    "BERT (introduced in [this paper](https://arxiv.org/abs/1810.04805)) stands for Bidirectional Encoder Representations from Transformers. If you don't know what most of that means - you've come to the right place! Let's unpack the main ideas:\n",
    "\n",
    "- Bidirectional - to understand the text  you're looking you'll have to look back (at the previous words) and forward (at the next words)\n",
    "- Transformers - The [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. `his` in a sentence refers to Jim).\n",
    "- (Pre-trained) contextualized word embeddings - [The ELMO paper](https://arxiv.org/abs/1802.05365v2) introduced a way to encode words based on their meaning/context. Nails has multiple meanings - fingernails and metal nails.\n",
    "\n",
    "BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence. Let's look at examples of these tasks:\n",
    "\n",
    "### Masked Language Modeling (Masked LM)\n",
    "\n",
    "The objective of this task is to guess the masked tokens. Let's look at an example, and try to not make it harder than it has to be:\n",
    "\n",
    "That's `[mask]` she `[mask]` -> That's what she said\n",
    "\n",
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "Given a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let's continue with the example:\n",
    "\n",
    "*Input* = `[CLS]` That's `[mask]` she `[mask]`. [SEP] Hahaha, nice! [SEP]\n",
    "\n",
    "*Label* = *IsNext*\n",
    "\n",
    "*Input* = `[CLS]` That's `[mask]` she `[mask]`. [SEP] Dwight, you ignorant `[mask]`! [SEP]\n",
    "\n",
    "*Label* = *NotNext*\n",
    "\n",
    "The training corpus was comprised of two entries: [Toronto Book Corpus](https://arxiv.org/abs/1506.06724) (800M words) and English Wikipedia (2,500M words). While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder.\n",
    "\n",
    "BERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large).\n",
    "\n",
    "### Is This Thing Useful in Practice?\n",
    "\n",
    "The BERT paper was released along with [the source code](https://github.com/google-research/bert) and pre-trained models.\n",
    "\n",
    "The best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmj22-TcZMef"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll need [the Transformers library](https://huggingface.co/transformers/) by Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kj_7Tz0-pK69"
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjsbi1u3QFEM"
   },
   "outputs": [],
   "source": [
    "# !pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "AJqoaFpVpoM8",
    "outputId": "88b5415f-9104-4937-c782-09c6025945c6"
   },
   "outputs": [],
   "source": [
    "# %reload_ext watermark\n",
    "# %watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7e65859fac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "# transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Setup & Config\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "We'll load the Google Play app reviews dataset, that we've put together in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4633.txt', '4548.txt', '2642.txt', '2022.txt', '3252.txt', '2379.txt', '2927.txt', '2525.txt', '2669.txt', '2934.txt', '3143.txt', '3359.txt', '4319.txt', '3161.txt', '2483.txt', '2594.txt', '3004.txt', '4049.txt', '3200.txt', '3237.txt', '3828.txt', '2104.txt', '3524.txt', '3014.txt', '2383.txt', '3626.txt', '3796.txt', '3541.txt', '2105.txt', '4321.txt', '3111.txt', '3257.txt', '3268.txt', '3921.txt', '3345.txt', '2226.txt', '2548.txt', '3530.txt', '2568.txt', '4366.txt', '2111.txt', '3464.txt', '4682.txt', '3313.txt', '2423.txt', '3596.txt', '4123.txt', '2293.txt', '2308.txt', '3801.txt', '3518.txt', '3382.txt', '2514.txt', '2957.txt', '2982.txt', '3328.txt', '3750.txt', '2962.txt', '4831.txt', '2828.txt', '3595.txt', '2888.txt', '3051.txt', '3253.txt', '4759.txt', '2611.txt', '2667.txt', '3102.txt', '2340.txt', '2469.txt', '2178.txt', '3809.txt', '3315.txt', '2967.txt', '2528.txt', '3207.txt', '3523.txt', '3007.txt', '3013.txt', '4483.txt', '3813.txt', '2062.txt', '3223.txt', '3028.txt', '3216.txt', '3097.txt', '4316.txt', '2558.txt', '3092.txt', '2842.txt', '4130.txt', '2078.txt', '3280.txt', '4330.txt', '3000.txt', '2744.txt', '3327.txt', '2608.txt', '3496.txt', '2373.txt', '3234.txt', '2175.txt', '3845.txt', '3167.txt', '3424.txt', '2300.txt', '3911.txt', '3245.txt', '3439.txt', '2287.txt', '4707.txt', '2405.txt', '4032.txt', '3343.txt', '2467.txt', '2154.txt', '2324.txt', '3115.txt', '2227.txt', '2840.txt', '3303.txt', '3399.txt', '2313.txt', '3551.txt', '3504.txt', '3099.txt', '2024.txt', '2185.txt', '2482.txt', '2921.txt', '2492.txt', '2040.txt', '2818.txt', '3283.txt', '2323.txt', '3232.txt', '2610.txt', '3570.txt', '2545.txt', '2445.txt', '4572.txt', '3144.txt', '3083.txt', '2039.txt', '4736.txt', '3615.txt', '4733.txt', '3908.txt', '3686.txt', '3663.txt', '2342.txt', '4726.txt', '4859.txt', '2566.txt', '4788.txt', '2597.txt', '4646.txt', '2950.txt', '4149.txt', '3064.txt', '3458.txt', '2932.txt', '2191.txt', '3310.txt', '2913.txt', '4174.txt', '4345.txt', '3658.txt', '2305.txt', '2301.txt', '3460.txt', '2749.txt', '3624.txt', '2436.txt', '2602.txt', '2122.txt', '3342.txt', '3556.txt', '4137.txt', '2676.txt', '3697.txt', '2770.txt', '3760.txt', '3174.txt', '2177.txt', '4812.txt', '2956.txt', '2041.txt', '2782.txt', '3018.txt', '3788.txt', '2460.txt', '4166.txt', '3852.txt', '4150.txt', '3738.txt', '2614.txt', '3226.txt', '4792.txt', '2472.txt', '2252.txt', '2630.txt', '2854.txt', '4181.txt', '3636.txt', '2163.txt', '3009.txt', '3688.txt', '2101.txt', '3011.txt', '3696.txt', '3372.txt', '2598.txt', '3803.txt', '2262.txt', '3284.txt', '4153.txt', '2751.txt', '4108.txt', '2421.txt', '3087.txt', '3454.txt', '2970.txt', '2909.txt', '4138.txt', '2930.txt', '3142.txt', '3021.txt', '4675.txt', '3398.txt', '4703.txt', '2130.txt', '3962.txt', '3016.txt', '4333.txt', '2800.txt', '2679.txt', '4019.txt', '3148.txt', '2260.txt', '3682.txt', '2490.txt', '2847.txt', '3567.txt', '4026.txt', '3720.txt', '3152.txt', '3282.txt', '2658.txt', '3246.txt', '2579.txt', '4608.txt', '3574.txt', '3449.txt', '2325.txt', '3798.txt', '3156.txt', '3304.txt', '2917.txt', '3056.txt', '3754.txt', '4917.txt', '3443.txt', '2803.txt', '2334.txt', '4077.txt', '2661.txt', '2549.txt', '4774.txt', '3074.txt', '2675.txt', '3457.txt', '3320.txt', '2617.txt', '2806.txt', '2110.txt', '2876.txt', '3745.txt', '4928.txt', '2102.txt', '2181.txt', '4519.txt', '3355.txt', '2355.txt', '2995.txt', '3294.txt', '2431.txt', '3825.txt', '2761.txt', '2794.txt', '3168.txt', '2309.txt', '4370.txt', '4716.txt', '3158.txt', '3543.txt', '3169.txt', '4630.txt', '4784.txt', '3190.txt', '4079.txt', '3703.txt', '4940.txt', '3040.txt', '2819.txt', '3642.txt', '4877.txt', '4565.txt', '2835.txt', '2562.txt', '2994.txt', '2109.txt', '3170.txt', '3124.txt', '2875.txt', '4379.txt', '3693.txt', '3154.txt', '4765.txt', '3068.txt', '4626.txt', '2372.txt', '2015.txt', '3747.txt', '3576.txt', '3360.txt', '2653.txt', '2279.txt', '3039.txt', '3805.txt', '3326.txt', '3090.txt', '3815.txt', '4728.txt', '4801.txt', '3019.txt', '4051.txt', '2266.txt', '3692.txt', '3597.txt', '2349.txt', '2792.txt', '3979.txt', '2035.txt', '4013.txt', '4618.txt', '2557.txt', '2120.txt', '2249.txt', '4659.txt', '4055.txt', '4642.txt', '3447.txt', '3130.txt', '4147.txt', '4023.txt', '3311.txt', '2137.txt', '2365.txt', '2793.txt', '3250.txt', '3655.txt', '2244.txt', '3208.txt', '3279.txt', '2008.txt', '2228.txt', '2025.txt', '3054.txt', '3527.txt', '2540.txt', '3587.txt', '2703.txt', '3231.txt', '2790.txt', '3203.txt', '2576.txt', '2926.txt', '3455.txt', '4311.txt', '2085.txt', '2413.txt', '2253.txt', '2275.txt', '2376.txt', '3983.txt', '3293.txt', '2999.txt', '3811.txt', '3638.txt', '3561.txt', '3309.txt', '4660.txt', '3569.txt', '3214.txt', '3113.txt', '4858.txt', '2290.txt', '2734.txt', '4720.txt', '2935.txt', '4840.txt', '4028.txt', '3384.txt', '2155.txt', '3029.txt', '4184.txt', '4826.txt', '4723.txt', '3647.txt', '2998.txt', '2064.txt', '2584.txt', '2524.txt', '4036.txt', '2684.txt', '2827.txt', '3151.txt', '3421.txt', '2849.txt', '2993.txt', '4099.txt', '3270.txt', '4745.txt', '3607.txt', '2283.txt', '4380.txt', '3050.txt', '3586.txt', '3386.txt', '2615.txt', '2020.txt', '2187.txt', '3286.txt', '3651.txt', '2232.txt', '3467.txt', '4064.txt', '2354.txt', '2220.txt', '4327.txt', '3728.txt', '4902.txt', '2157.txt', '3076.txt', '3331.txt', '3584.txt', '2989.txt', '3414.txt', '2929.txt', '2476.txt', '3041.txt', '3463.txt', '2627.txt', '3855.txt', '2362.txt', '2952.txt', '2707.txt', '2965.txt', '3525.txt', '2619.txt', '3639.txt', '3255.txt', '3173.txt', '3429.txt', '4347.txt', '2586.txt', '3049.txt', '3238.txt', '3103.txt', '3709.txt', '4177.txt', '3272.txt', '4155.txt', '3557.txt', '2692.txt', '3166.txt', '3883.txt', '3985.txt', '3549.txt', '2302.txt', '2072.txt', '3081.txt', '3409.txt', '2263.txt', '3275.txt', '2502.txt', '3086.txt', '2759.txt', '3131.txt', '3233.txt', '3680.txt', '2316.txt', '2241.txt', '3134.txt', '2879.txt', '3406.txt', '3431.txt', '2184.txt', '2051.txt', '2834.txt', '3711.txt', '4829.txt', '3346.txt', '3515.txt', '3373.txt', '4349.txt', '3405.txt', '2773.txt', '2559.txt', '2488.txt', '2554.txt', '3495.txt', '3052.txt', '3965.txt', '3428.txt', '2634.txt', '2768.txt', '2983.txt', '2071.txt', '3138.txt', '3118.txt', '3952.txt', '3476.txt', '2387.txt', '3764.txt', '3784.txt', '2264.txt', '3420.txt', '3433.txt', '3506.txt', '3821.txt', '2479.txt', '2435.txt', '2094.txt', '4037.txt', '3256.txt', '3662.txt', '3204.txt', '3841.txt', '3062.txt', '2397.txt', '4785.txt', '3917.txt', '2657.txt', '3362.txt', '2870.txt', '3146.txt', '3247.txt', '3030.txt', '3186.txt', '3121.txt', '2884.txt', '4048.txt', '3707.txt', '3088.txt', '4799.txt', '3804.txt', '3403.txt', '2663.txt', '3135.txt', '3554.txt', '2772.txt', '3107.txt', '2304.txt', '3773.txt', '2826.txt', '3387.txt', '3185.txt', '3265.txt', '4175.txt', '2984.txt', '2495.txt', '2599.txt', '3988.txt', '2537.txt', '4698.txt', '2314.txt', '2647.txt', '2662.txt', '4341.txt', '2032.txt', '4339.txt', '2336.txt', '4649.txt', '3802.txt', '2788.txt', '2652.txt', '2231.txt', '4876.txt', '2012.txt', '4033.txt', '4814.txt', '3393.txt', '4072.txt', '2451.txt', '3181.txt', '4334.txt', '4336.txt', '3533.txt', '4362.txt', '3319.txt', '4090.txt', '3375.txt', '3956.txt', '2839.txt', '2160.txt', '4927.txt', '3344.txt', '2620.txt', '2292.txt', '2010.txt', '2570.txt', '4617.txt', '3902.txt', '2821.txt', '2575.txt', '3215.txt', '3195.txt', '4802.txt', '2295.txt', '2149.txt', '3847.txt', '4735.txt', '2871.txt', '2539.txt', '4071.txt', '2289.txt', '3500.txt', '2552.txt', '3389.txt', '3227.txt', '3175.txt', '2585.txt', '2485.txt', '3365.txt', '2285.txt', '4329.txt', '3244.txt', '3162.txt', '3012.txt', '2268.txt', '3155.txt', '2723.txt', '2672.txt', '2968.txt', '2741.txt', '4038.txt', '3769.txt', '3349.txt', '3184.txt', '2457.txt', '2578.txt', '2038.txt', '3735.txt', '2303.txt', '2640.txt', '3870.txt', '3330.txt', '3354.txt', '2690.txt', '3509.txt', '3171.txt', '3047.txt', '2407.txt', '2547.txt', '3777.txt', '2418.txt', '2073.txt', '4171.txt', '3266.txt', '4830.txt', '2837.txt', '3225.txt', '2587.txt', '2546.txt', '2205.txt', '4353.txt', '4060.txt', '4360.txt', '3045.txt', '3364.txt', '2506.txt', '4148.txt', '2991.txt', '3427.txt', '2092.txt', '3340.txt', '2571.txt', '3063.txt', '3797.txt', '3325.txt', '2866.txt', '4709.txt', '3487.txt', '2862.txt', '4159.txt', '3300.txt', '4681.txt', '2382.txt', '2774.txt', '3254.txt', '3069.txt', '2466.txt', '3419.txt', '4082.txt', '2180.txt', '3288.txt', '2565.txt', '2125.txt', '3334.txt', '3189.txt', '2954.txt', '3770.txt', '4752.txt', '3235.txt', '2631.txt', '4644.txt', '3071.txt', '2953.txt', '2439.txt', '2515.txt', '3317.txt', '3441.txt', '2331.txt', '4679.txt', '3194.txt', '2756.txt', '3736.txt', '3020.txt', '4363.txt', '3198.txt', '2296.txt', '3734.txt', '2095.txt', '2366.txt', '4092.txt', '3080.txt', '4443.txt', '3065.txt', '3497.txt', '4688.txt', '3140.txt', '4376.txt', '3517.txt', '4611.txt', '3236.txt', '2501.txt', '4770.txt', '3539.txt', '3015.txt', '2780.txt', '3725.txt', '2960.txt', '2767.txt', '3242.txt', '2893.txt', '4856.txt', '3276.txt', '3526.txt', '2543.txt', '3408.txt', '3230.txt', '3353.txt', '2534.txt', '3781.txt', '2944.txt', '3453.txt', '2079.txt', '3887.txt', '2452.txt', '2650.txt', '2717.txt', '3001.txt', '2877.txt', '3201.txt', '2963.txt', '2981.txt', '2053.txt', '2898.txt', '2090.txt', '2395.txt', '3196.txt', '2370.txt', '3591.txt', '2499.txt', '3219.txt', '2820.txt', '4109.txt', '2432.txt', '2604.txt', '3746.txt', '2093.txt', '3723.txt', '2736.txt', '4022.txt', '3061.txt', '3333.txt', '4152.txt', '2766.txt', '3002.txt', '3993.txt', '2868.txt', '4378.txt', '2406.txt', '2519.txt', '3228.txt', '2785.txt', '3352.txt', '2234.txt', '3716.txt', '3267.txt', '2789.txt', '4364.txt', '3435.txt', '3367.txt', '3205.txt', '3034.txt', '3580.txt', '3537.txt', '4320.txt', '2689.txt', '2754.txt', '4080.txt', '3046.txt', '2603.txt', '4114.txt', '4325.txt', '3633.txt', '3426.txt', '3694.txt', '3073.txt', '3059.txt', '2446.txt', '4666.txt', '4078.txt', '2709.txt', '3838.txt', '4004.txt', '2450.txt', '3296.txt', '2874.txt', '2775.txt', '3628.txt', '4127.txt', '3565.txt', '2006.txt', '3862.txt', '3727.txt', '3057.txt', '3070.txt', '3508.txt', '3763.txt', '3338.txt', '2065.txt', '4056.txt', '3077.txt', '2959.txt', '2844.txt', '2265.txt', '2521.txt', '3136.txt', '4154.txt', '3187.txt', '3563.txt', '2471.txt', '2648.txt', '2945.txt', '2344.txt', '2910.txt', '2504.txt', '2113.txt', '2851.txt', '2526.txt', '2510.txt', '4655.txt', '3085.txt', '2247.txt', '3239.txt', '3383.txt', '3206.txt', '2942.txt', '4796.txt', '4103.txt', '3830.txt', '2797.txt', '2005.txt', '2389.txt', '2353.txt', '3743.txt', '3351.txt', '3133.txt', '3606.txt', '3182.txt', '3402.txt', '4725.txt', '4168.txt', '3259.txt', '3260.txt', '2380.txt', '3665.txt', '3550.txt', '3411.txt', '4382.txt', '3369.txt', '4312.txt', '4050.txt', '3925.txt', '3521.txt', '3271.txt', '2429.txt', '4890.txt', '3971.txt', '2860.txt', '2710.txt', '3659.txt', '4721.txt', '3003.txt', '3657.txt', '4358.txt', '3898.txt', '4605.txt', '2693.txt', '3072.txt', '3417.txt', '3675.txt', '2339.txt', '3363.txt', '3810.txt', '2086.txt', '4886.txt', '2237.txt', '2330.txt', '2248.txt', '4615.txt', '3448.txt', '2623.txt', '3699.txt', '2171.txt', '3291.txt', '2511.txt', '4342.txt', '3751.txt', '3791.txt', '2609.txt', '4074.txt', '2465.txt', '4340.txt', '4165.txt', '2996.txt', '4697.txt', '4158.txt', '2678.txt', '4314.txt', '2593.txt', '3290.txt', '2124.txt', '2755.txt', '3425.txt', '3082.txt', '3324.txt', '3445.txt', '3513.txt', '2018.txt', '2533.txt', '2622.txt', '3379.txt', '2424.txt', '3450.txt', '2455.txt', '2486.txt', '2299.txt', '2107.txt', '2719.txt', '2992.txt', '2729.txt', '2915.txt', '3381.txt', '2426.txt', '3159.txt', '3660.txt', '2028.txt', '2139.txt', '2889.txt', '2883.txt', '2221.txt', '4113.txt', '4346.txt', '2691.txt', '2938.txt', '2900.txt', '3306.txt', '3676.txt', '3229.txt', '3946.txt', '3903.txt', '2589.txt', '3646.txt', '2955.txt', '3095.txt', '3377.txt', '2858.txt', '3361.txt', '2726.txt', '4758.txt', '3150.txt', '3514.txt', '2638.txt', '4905.txt', '3120.txt', '2190.txt', '3093.txt', '3188.txt', '3368.txt', '2711.txt', '2060.txt', '4603.txt', '2887.txt', '3023.txt', '4104.txt', '4372.txt', '2628.txt', '3691.txt', '4822.txt', '4691.txt', '3850.txt', '3768.txt', '3251.txt', '3573.txt', '3397.txt', '4356.txt', '4133.txt', '3104.txt', '2478.txt', '2019.txt', '3042.txt', '2027.txt', '2924.txt', '2784.txt', '3108.txt', '3926.txt', '2527.txt', '2477.txt', '4129.txt', '4628.txt', '2713.txt', '4821.txt', '2743.txt', '2437.txt', '2641.txt', '2259.txt', '3221.txt', '3281.txt', '2061.txt', '2776.txt', '3075.txt', '3036.txt', '4101.txt', '4008.txt', '3489.txt', '2393.txt', '3876.txt', '3332.txt', '2427.txt', '4834.txt', '2399.txt', '3535.txt', '3666.txt', '4908.txt', '2145.txt', '3774.txt', '4868.txt', '2448.txt', '3096.txt', '4619.txt', '2368.txt', '2442.txt', '2197.txt', '2386.txt', '3038.txt', '2708.txt', '2067.txt', '3105.txt', '2716.txt', '3681.txt', '2168.txt', '4096.txt', '2832.txt', '2969.txt', '3067.txt', '3503.txt', '2812.txt', '3025.txt', '4880.txt', '3055.txt', '3371.txt', '2621.txt', '3451.txt', '3776.txt', '2278.txt', '4936.txt', '3269.txt', '2896.txt', '3491.txt', '2645.txt', '4151.txt', '2830.txt', '2235.txt', '3202.txt', '3191.txt', '2897.txt', '2433.txt', '4318.txt', '2616.txt', '3473.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "my_dir_path = \"Switchboard-Corpus/swda_data/train\"\n",
    "\n",
    "results = defaultdict(list)\n",
    "for _,_,file in os.walk(my_dir_path):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_dir_path = \"Switchboard-Corpus/swda_data/train/\"\n",
    "\n",
    "results = defaultdict(list)\n",
    "temp = True\n",
    "for file in os.listdir(my_dir_path):\n",
    "    file_path= my_dir_path + file\n",
    "    file_open = open(file_path, \"r\")\n",
    "    file_text = file_open.read()\n",
    "    lines = file_text.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        results[\"file_name\"].append(file)\n",
    "        line_split = line.split(\"|\")\n",
    "        results[\"utterances\"].append(line_split[1])\n",
    "        results['dialogue_acts'].append(line_split[2])\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dB2jE6am7Dpo",
    "outputId": "19acca28-2336-43f1-b714-29d993bc114c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192415, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>utterances</th>\n",
       "      <th>dialogue_acts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4633.txt</td>\n",
       "      <td>How many children do you have now?</td>\n",
       "      <td>qw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4633.txt</td>\n",
       "      <td>Well I have five.</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4633.txt</td>\n",
       "      <td>You have five?</td>\n",
       "      <td>br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4633.txt</td>\n",
       "      <td>Right.</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4633.txt</td>\n",
       "      <td>Well you do have a hard time finding time  the...</td>\n",
       "      <td>qy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192410</th>\n",
       "      <td>3473.txt</td>\n",
       "      <td>I, you could,</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192411</th>\n",
       "      <td>3473.txt</td>\n",
       "      <td>but, uh, I don't mind, uh, you know,</td>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192412</th>\n",
       "      <td>3473.txt</td>\n",
       "      <td>if I do like sit ups, I'll throw a, a towel do...</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192413</th>\n",
       "      <td>3473.txt</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192414</th>\n",
       "      <td>3473.txt</td>\n",
       "      <td>so you don't have the abrasive,</td>\n",
       "      <td>sd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name                                         utterances  \\\n",
       "0       4633.txt                 How many children do you have now?   \n",
       "1       4633.txt                                  Well I have five.   \n",
       "2       4633.txt                                     You have five?   \n",
       "3       4633.txt                                             Right.   \n",
       "4       4633.txt  Well you do have a hard time finding time  the...   \n",
       "...          ...                                                ...   \n",
       "192410  3473.txt                                      I, you could,   \n",
       "192411  3473.txt               but, uh, I don't mind, uh, you know,   \n",
       "192412  3473.txt  if I do like sit ups, I'll throw a, a towel do...   \n",
       "192413  3473.txt                                              Yeah.   \n",
       "192414  3473.txt                    so you don't have the abrasive,   \n",
       "\n",
       "       dialogue_acts  \n",
       "0                 qw  \n",
       "1                 sd  \n",
       "2                 br  \n",
       "3                 aa  \n",
       "4                 qy  \n",
       "...              ...  \n",
       "192410            sv  \n",
       "192411             %  \n",
       "192412            sd  \n",
       "192413             b  \n",
       "192414            sd  \n",
       "\n",
       "[192415 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWqVNHJbn10l"
   },
   "source": [
    "We have about 16k examples. Let's check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "VA_wGSLQLKCh",
    "outputId": "9468ddba-47d0-46a8-d7bb-6ff51c4bdb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192415 entries, 0 to 192414\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   file_name      192415 non-null  object\n",
      " 1   utterances     192415 non-null  object\n",
      " 2   dialogue_acts  192415 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3cL_1qVn_6h"
   },
   "source": [
    "Great, no missing values in the score and review texts! Do we have class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "id": "Wwh_rW4Efhs3",
    "outputId": "e39b9955-3c5e-45f3-f960-38bfa03447c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABeIAAAPTCAYAAADciXkAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAABYlAAAWJQFJUiTwAADSjklEQVR4nOz9fZhV5WEu/t8bhxflTQgvlkQarUp0qpCGpjFSRKGnP5NwfDnVoo2NBiRgEhI1PTHHeIwxLeb0cKStVU8Yi7Gp0qR14ktiWqNEETGtpIIHCNGWGJQ43TgMCAjMOPP9g2v2b942jOISg5/PdXFda+9nPfd61sa/bpbPKrW1tbUFAAAAAAAoRJ+DvQAAAAAAADiUKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKVHOwF8Chbe3atdm9e3cOO+yw9O/f/2AvBwAAAADgDdm9e3dee+219O/fPyeddNLrmquIp1C7d+9Oa2trWltb09zcfLCXAwAAAABwQHbv3v265yjiKdRhhx2W1tbW9OnTJ0ccccTBXg4AAAAAwBuyc+fOtLa25rDDDnvdcxXxFKp///5pbm7OEUcckXHjxh3s5QAAAAAAvCHr16/P9u3b39AW3F7WCgAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQoJqDvQA4GMq33VZo/sg5cwrNBwAAAAB+dXgiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIAC1RzsBRThzDPPzIsvvtjr8z/zmc/ks5/9bLfvW1pasmTJktx///3ZsGFD9uzZkzFjxmTatGm55JJLMnz48P1mNzY25o477sgPf/jDbNq0Kf369csxxxyT6dOnZ8aMGamp2f9fwfr16/PNb34zK1asyObNmzN06NDU1tZmxowZOeOMM3p1j0uXLs2SJUuyZs2abN26NSNGjMipp56aT3ziExk3blyvMgAAAAAAeP1KbW1tbQd7EW+211vE/+Vf/mV+//d/v9N3r7zySmbOnJlVq1b1OGfkyJFZtGhRTjzxxKq5a9euzezZs1Mul3scnzBhQurq6jJ48OCqGfX19bn22mvT3Nzc4/iFF16Yr3zlK1XnJ8l1112XJUuW9DjWr1+/3HDDDTnnnHP2mfFGrV+/Ptu3b8+gQYPeVoV/+bbbCs0fOWdOofkAAAAAwFvrQLrOQ3Jrmu9973v5yU9+ss8/7QX60KFDe3yq/Morr8yqVatSKpUyZ86cPPTQQ1m2bFnmz5+fwYMHp1wu51Of+lSampp6XENTU1PmzJmTcrmcIUOGZP78+Vm2bFkeeuihzJkzJ6VSKU8//XSuvPLKqvexcuXKfPnLX05zc3NOOOGE3H777VmxYkXuueeeTJs2LUly9913Z9GiRVUzFi1aVCnhp02blnvuuScrVqzI7bffnhNOOCF79uzJNddck5UrV/b25wUAAAAA4HU4JIv4ww8/PAMHDqz656WXXsq6deuSJGeddVb69evXaf6jjz6axx57LEnyuc99LldccUXGjh2bUaNG5bzzzsttt92WUqmUhoaG1NXV9biGRYsWpaGhIaVSKbfeemvOO++8jBo1KmPHjs0VV1yRz33uc0mSxx57rHKtrm688ca0tLRkxIgRufPOOzNp0qQMHz48tbW1ufnmm3PaaaclSW655ZY0NjZ2m9/Y2JhbbrklSTJp0qTcfPPNqa2tzfDhwzNp0qTceeedGTFiRFpaWvL1r3/9DfzSAAAAAADszyFZxO/Pd7/73crxueee2238rrvuSpIMGzYsM2fO7DY+ceLETJkyJUnyne98Jy0tLZ3GW1pa8u1vfztJMmXKlEycOLFbxsyZM3PkkUd2ul5HzzzzTFavXp0kmTVrVoYNG9ZpvFQq5aqrrkqS7Ny5M/fee2+3jPr6+uzcuTPJ3if8S6VSp/Fhw4Zl1qxZSZJVq1ZlzZo13TIAAAAAADgw77givq2tLffff3+S5L3vfW8mTJjQaXzXrl1ZsWJFkmTq1KndnpZvd9ZZZyXZuwVN121dnnrqqWzbtq3TeV3169evsr3ME088kV27dnUaX7p0abdrdVVbW5uxY8cmSR555JFu4+0ZY8eOTW1t7T7vo1oGAAAAAAAH5h1XxD/55JP55S9/mSQ5++yzu40/++yz2b17d5J0K+k76jjW9Unyjp97k7F79+4899xzPWaMHj06Rx11VNWM8ePH97iGjt+1n9OTo446KqNHj66aAQAAAADAgXnHFfHtW7iUSqUei/gNGzZUjt/znvdUzRkzZkz69OnTbU7Hz3369MmYMWOqZnTMr5Zx9NFHV53fMWPHjh1paGiofN/Q0FDZlqa3GV3XAAAAAADAgas52At4K7366qv5p3/6pyTJb//2b+fd7353t3O2bNlSOX7Xu95VNatv374ZMmRImpqa0tTU1GPGkCFD0rdv36oZw4cPrxxXy9jXGrqONzU1VZ5u7+19dBzvuoY30/bt27tt4XMwfOADH3hLr/d2uGcAAAAA4OB6Rz0R/8///M+Vp8TPOeecHs959dVXK8f9+/ffZ177eHtm14z9zR8wYEDluFpGtT3q95fR8bi397Fjx459ngcAAAAAwOv3jnoi/r777kuSHH744fn93//9g7yad5ZBgwZl3LhxB3sZb7m3+gl8AAAAAKAY69evz/bt29/Q3HfME/H/+Z//mRUrViRJpk6dmkGDBvV43uGHH145bn9pazXt40cccUSPGfubv2vXrspxtYw9e/a8oYyOx729j4EDB+7zPAAAAAAAXr93TBF/33335bXXXkuSnHvuuVXPGzZsWOX45Zdfrnpec3Nztm3bliQ58sgje8zYtm1bWlpaqmY0NjZWjqtl7GsNXcc7ZvT2PjqOd10DAAAAAAAH7h1TxN97771JklGjRuXDH/5w1fOOOeaYyvELL7xQ9bxNmzaltbW125yOn1tbW/Piiy9WzeiYXy1j48aNVed3zBg4cGDlRa3J3vtsfyq+txld1wAAAAAAwIF7RxTxa9euzc9+9rMkyfTp09OnT/XbPv744ysvL121alXV855++unKcW1tbaexjp97k9G/f/8cd9xxPWY0NDSkoaGhakZ7ftc1lEqlynerV6+uOv+ll16q5HfNAAAAAADgwL0jivj2p+GT5JxzztnnuQMGDMipp56aJHn44Yer7tH+gx/8IMne7Vy6vpBz4sSJGTJkSKfzutqzZ08eeeSRJMmHP/zhDBgwoNP4GWecUTl+8MEHe8xYu3ZtfvGLXyRJzjzzzG7j7RnPP/981q1bt8/7qJYBAAAAAMCBOeSL+Ndeey0PPPBAkr1PfJ9wwgn7nXPRRRcl2buH++LFi7uNr1y5Mj/60Y+SJOeff35qamo6jdfU1OSCCy5IkixdujQrV67slrF48eLKHvHt1+vo5JNPzimnnJIkqaurS1NTU6fxtra2LFiwIMneF7OeffbZ3TLOPffcyvY0CxYsSFtbW6fxpqam1NXVJUnGjx/viXgAAAAAgAIc8kX8448/ns2bNydJj2V1T04//fRMnjw5SbJw4cIsXLgwGzduTLlcTn19febOnZvW1taMHj06s2bN6jHjsssuy+jRo9Pa2pq5c+emvr4+5XI5GzduzE033ZSFCxcmSSZPnly5VldXX311ampqUi6Xc/HFF2f58uVpbGzMunXrMm/evDz++ONJkssvvzzDhw/vNn/48OG5/PLLkyTLli3LvHnzsm7dujQ2Nmb58uW5+OKLUy6XU1NTky9+8Yu9+m0AAAAAAHh9Sm1dH5M+xFxxxRX5/ve/n5qamixbtqzHwron27Zty6xZs6ru8T5y5MgsWrQoJ554YtWMtWvXZvbs2SmXyz2OT5gwIXV1dRk8eHDVjPr6+lx77bVpbm7ucXzGjBm5/vrr93EnyXXXXZclS5b0ONa3b9987Wtf2++WPW/U+vXrs3379gwaNCjjxo0r5BpvRPm22wrNHzlnTqH5AAAAAMBb60C6zpr9n/Kra/v27ZV92H/3d3+31yV8kgwZMiR33XVXlixZkvvuuy8bNmxIc3NzxowZk6lTp+bSSy/db95JJ52U++67L4sXL87DDz+cTZs2pW/fvjn22GMzffr0zJgxo9u2Nl2de+65Oemkk3LHHXfkySefTLlcztChQ1NbW5sLL7yw017y1Vx//fWZMmVK7r777qxZsyZbt27NyJEj86EPfSiXXHLJ26ogBwAAAAA41BzyT8RzcHkiHgAAAAA4FBxI13nI7xEPAAAAAAAHkyIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKFDNwV7AW+HJJ59MfX19Vq5cmXK5nH79+mXkyJE5+eSTc/rpp+cjH/lIj/NaWlqyZMmS3H///dmwYUP27NmTMWPGZNq0abnkkksyfPjw/V67sbExd9xxR374wx9m06ZN6devX4455phMnz49M2bMSE3N/v8K1q9fn29+85tZsWJFNm/enKFDh6a2tjYzZszIGWec0avfYOnSpVmyZEnWrFmTrVu3ZsSIETn11FPziU98IuPGjetVBgAAAAAAr1+pra2t7WAvoii7du3KNddckwceeKDqOe9+97vzyCOPdPv+lVdeycyZM7Nq1aoe540cOTKLFi3KiSeeWDV77dq1mT17dsrlco/jEyZMSF1dXQYPHlw1o76+Ptdee22am5t7HL/wwgvzla98per8JLnuuuuyZMmSHsf69euXG264Ieecc84+M96o9evXZ/v27Rk0aNDbqvAv33Zbofkj58wpNB8AAAAAeGsdSNd5yG5N09LSkk9/+tN54IEH0rdv33ziE5/It7/97axYsSLLly/Pt771rXzyk5/MqFGjepx/5ZVXZtWqVSmVSpkzZ04eeuihLFu2LPPnz8/gwYNTLpfzqU99Kk1NTT3Ob2pqypw5c1IulzNkyJDMnz8/y5Yty0MPPZQ5c+akVCrl6aefzpVXXln1HlauXJkvf/nLaW5uzgknnJDbb789K1asyD333JNp06YlSe6+++4sWrSoasaiRYsqJfy0adNyzz33ZMWKFbn99ttzwgknZM+ePbnmmmuycuXKXv6yAAAAAAC8HofsE/Hf+MY3smDBgvTv3z+LFi3K7/zO7/R67qOPPprZs2cnST7/+c9n7ty5ncafeuqpfPzjH09bW1suu+yyfOELX+iW8ed//uepq6tLqVTKt771rUycOLHT+K233pqFCxcm2VuWT548uVvG+eefn9WrV2fEiBF54IEHMmzYsMpYW1tbZs6cmeXLl+eII47Iww8/3G2rnMbGxkydOjU7d+7MpEmTKutpt2XLlnzsYx/L5s2bM378+Hz729/u9W/UW56IBwAAAAAOBZ6I72Lr1q3567/+6yTJnDlzXlcJnyR33XVXkmTYsGGZOXNmt/GJEydmypQpSZLvfOc7aWlp6TTe0tJSKbWnTJnSrYRPkpkzZ+bII4/sdL2OnnnmmaxevTpJMmvWrE4lfJKUSqVcddVVSZKdO3fm3nvv7ZZRX1+fnTt3Jtn7hH/HEr79/mbNmpUkWbVqVdasWdMtAwAAAACAA3NIFvH33Xdfdu3alb59++aP/uiPXtfcXbt2ZcWKFUmSqVOnpl+/fj2ed9ZZZyXZuwVN121dnnrqqWzbtq3TeV3169evsr3ME088kV27dnUaX7p0abdrdVVbW5uxY8cmSY/73LdnjB07NrW1tfu8j2oZAAAAAAAcmEOyiH/00UeTJL/5m7+ZoUOHVr5/7bXX0traus+5zz77bHbv3p1k78tUq+k41vVJ8o6fe5Oxe/fuPPfccz1mjB49OkcddVTVjPHjx/e4ho7ftZ/Tk6OOOiqjR4+umgEAAAAAwIE5JIv4//f//l+S5LjjjsuePXvyjW98I2eddVZOPvnk1NbWZtq0afna176Wl156qdvcDRs2VI7f8573VL3GmDFj0qdPn25zOn7u06dPxowZUzWjY361jKOPPrrq/I4ZO3bsSENDQ+X7hoaGyrY0vc3ougYAAAAAAA7cIVfE79q1K1u2bEmS9O3bNx//+MezYMGC/Md//EflifiNGzfmb//2bzN9+vQ8+eSTnea3z02Sd73rXVWv07dv3wwZMiTJ3u1pesoYMmRI+vbtWzWj48tVq2Xsaw1dxztm9PY+Oo53XQMAAAAAAAeu5mAv4M32yiuvVI6/853vpLm5OVOnTs1nP/vZ/MZv/EaamprywAMP5Kabbsq2bdsyb9683HfffZXtX1599dXK/P79++/zWu3j7U+et2vP2N/8AQMGVI6rZVTbo35/GR2Pe3sfO3bs2Od5B2L79u3d9tI/GD7wgQ+8pdd7O9wzAAAAAHBwHXJPxHfcA765uTmnn356/vqv/zonnnhi+vXrl1GjRuWTn/xkvv71rydJtm7dmrq6uoO1XAAAAAAADnGH3BPxAwcO7PT5M5/5TEqlUrfzPvKRj+TWW2/Nz372szz88MP58pe/nCQ5/PDDK+e0v7S1mvbxI444otP37Rn7m79r167KcU8Zzc3N2bNnzxvK6Hjc2/vo+tu9mQYNGpRx48YVlv929VY/gQ8AAAAAFGP9+vXZvn37G5p7yD0RP3DgwMp2LgMGDMhv/uZvVj134sSJSZJNmzZVtmUZNmxYZfzll1+uOre5uTnbtm1Lkhx55JGdxtoztm3blpaWlqoZjY2NleNqGftaQ9fxjhm9vY+O413XAAAAAADAgTvkivhSqZT3vve9SZLBgwenT5/qt9j+stUklX/JOOaYYyrfvfDCC1Xnbtq0qbINTsc5HT+3trbmxRdfrJrRMb9axsaNG6vO75gxcODAjB49uvL9qFGjKk/F9zaj6xoAAAAAADhwh1wRnyQnn3xykr1PpHfcM76rpqamyvHgwYOTJMcff3zl5aWrVq2qOvfpp5+uHNfW1nYa6/i5Nxn9+/fPcccd12NGQ0NDGhoaqma053ddQ6lUqny3evXqqvNfeumlSn7XDAAAAAAADtwhWcRPnTo1yd69z/dVhP/rv/5rkuS9731v5enxAQMG5NRTT02SPPzww1X3aP/BD36QZO92Ll33AZ84cWLlafv287ras2dPHnnkkSTJhz/84QwYMKDT+BlnnFE5fvDBB3vMWLt2bX7xi18kSc4888xu4+0Zzz//fNatW7fP+6iWAQAAAADAgTkki/jJkydn7NixSZK/+Iu/yGuvvdbtnPr6+vz7v/97kr0vbu3ooosuSrJ3D/fFixd3m7ty5cr86Ec/SpKcf/75qanp/M7bmpqaXHDBBUmSpUuXZuXKld0yFi9eXNkjvv16HZ188sk55ZRTkiR1dXWdnt5Pkra2tixYsCDJ3heznn322d0yzj333Mo/MCxYsCBtbW2dxpuamlJXV5ckGT9+vCfiAQAAAAAKcEgW8X379s3/+B//I6VSKStWrMhll12WlStXpqmpKc8//3xuvvnmXHvttUmSd7/73bn00ks7zT/99NMzefLkJMnChQuzcOHCbNy4MeVyOfX19Zk7d25aW1szevTozJo1q8c1XHbZZRk9enRaW1szd+7c1NfXp1wuZ+PGjbnpppuycOHCJHv/0aD9Wl1dffXVqampSblczsUXX5zly5ensbEx69aty7x58/L4448nSS6//PIMHz682/zhw4fn8ssvT5IsW7Ys8+bNy7p169LY2Jjly5fn4osvTrlcTk1NTb74xS++/h8aAAAAAID9KrV1fUz6EHLXXXflz/7sz9Lc3Nzj+NFHH53/+3//b37jN36j29i2bdsya9asqlvbjBw5MosWLcqJJ55Y9fpr167N7NmzUy6XexyfMGFC6urqKvvT96S+vj7XXntt1XuYMWNGrr/++qrzk+S6667LkiVLehzr27dvvva1r+Wcc87ZZ8YbtX79+mzfvj2DBg3KuHHjCrnGG1G+7bZC80fOmVNoPgAAAADw1jqQrvOQLuKT5Kc//WnuvPPOPPnkkymXy+nfv3+OPfbY/Jf/8l9y0UUXVbZu6UlLS0uWLFmS++67Lxs2bEhzc3PGjBmTqVOn5tJLL+3xKfSu2re3efjhh7Np06b07ds3xx57bKZPn54ZM2Z029amJ+vXr88dd9xRuYehQ4emtrY2F154Yae95Pdl6dKlufvuu7NmzZps3bo1I0eOzIc+9KFccsklhRbkingAAAAA4FCgiOdtSxEPAAAAABwKDqTrPCT3iAcAAAAAgLcLRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQoJqDvYA32wsvvJCpU6f26twVK1Zk+PDhPY61tLRkyZIluf/++7Nhw4bs2bMnY8aMybRp03LJJZdUnddRY2Nj7rjjjvzwhz/Mpk2b0q9fvxxzzDGZPn16ZsyYkZqa/f/869evzze/+c2sWLEimzdvztChQ1NbW5sZM2bkjDPO6NV9Ll26NEuWLMmaNWuydevWjBgxIqeeemo+8YlPZNy4cb3KAAAAAADgjSm1tbW1HexFvJnejCL+lVdeycyZM7Nq1aoe540cOTKLFi3KiSeeWDV77dq1mT17dsrlco/jEyZMSF1dXQYPHlw1o76+Ptdee22am5t7HL/wwgvzla98per8JLnuuuuyZMmSHsf69euXG264Ieecc84+Mw7E+vXrs3379gwaNOhtVfqXb7ut0PyRc+YUmg8AAAAAvLUOpOs8pLem+cY3vpGf/OQnVf9Ue6r9yiuvzKpVq1IqlTJnzpw89NBDWbZsWebPn5/BgwenXC7nU5/6VJqamnqc39TUlDlz5qRcLmfIkCGZP39+li1bloceeihz5sxJqVTK008/nSuvvLLq2leuXJkvf/nLaW5uzgknnJDbb789K1asyD333JNp06YlSe6+++4sWrSoasaiRYsqJfy0adNyzz33ZMWKFbn99ttzwgknZM+ePbnmmmuycuXKXv6iAAAAAAC8Xod0ET9gwIAMHDiw6p+ePProo3nssceSJJ/73OdyxRVXZOzYsRk1alTOO++83HbbbSmVSmloaEhdXV2PGYsWLUpDQ0NKpVJuvfXWnHfeeRk1alTGjh2bK664Ip/73OeSJI899ljlWl3deOONaWlpyYgRI3LnnXdm0qRJGT58eGpra3PzzTfntNNOS5LccsstaWxs7Da/sbExt9xyS5Jk0qRJufnmm1NbW5vhw4dn0qRJufPOOzNixIi0tLTk61//+uv7YQEAAAAA6LVDuoh/I+66664kybBhwzJz5sxu4xMnTsyUKVOSJN/5znfS0tLSabylpSXf/va3kyRTpkzJxIkTu2XMnDkzRx55ZKfrdfTMM89k9erVSZJZs2Zl2LBhncZLpVKuuuqqJMnOnTtz7733dsuor6/Pzp07k+x9wr9UKnUaHzZsWGbNmpUkWbVqVdasWdMtAwAAAACAA6eI72DXrl1ZsWJFkmTq1Knp169fj+edddZZSfZuQdN1W5ennnoq27Zt63ReV/369atsL/PEE09k165dncaXLl3a7Vpd1dbWZuzYsUmSRx55pNt4e8bYsWNTW1u7z/uolgEAAAAAwIF7RxTxe/bs6dV5zz77bHbv3p1k78tUq+k41vVJ8o6fe5Oxe/fuPPfccz1mjB49OkcddVTVjPHjx/e4ho7ftZ/Tk6OOOiqjR4+umgEAAAAAwIGrOdgLKNINN9yQF198MTt37ky/fv3y3ve+N7/7u7+bP/7jP+6x4N6wYUPl+D3veU/V3DFjxqRPnz5pbW3tNKdjRp8+fTJmzJiqGR3zN2zYkN/8zd/slnH00Ufv8/7aM3bs2JGGhoZKqd7Q0FDZlqY3GQ0NDd3uAwAAAACAN8ch/UT8s88+Wymk9+zZk5/97Ge5/fbbc9ZZZ+V73/tet/O3bNlSOX7Xu95VNbdv374ZMmRIkr3b0/SUMWTIkPTt27dqxvDhwyvH1TL2tYau4x0zensfHce7rgEAAAAAgDfHIfdEfJ8+fTJp0qR89KMfTW1tbX7t134t/fv3z/PPP5/vfe97+Zu/+Zvs3Lkzf/Inf5KhQ4dm0qRJlbmvvvpq5bh///77vE77eHvR3zVjf/MHDBhQOa6WUW2P+v1ldDzu7X3s2LFjn+cdqO3bt3fbT/9g+MAHPvCWXu/tcM8AAAAAwMF1yBXxY8aMye23397t+xNOOCEnnHBCTj/99FxyySXZvXt3brjhhnz/+9/PYYcddhBWCgAAAADAO8EhV8Tvz2/91m/l4osvTl1dXX7+859n9erVef/7358kOfzwwyvntb+0tZr28SOOOKLT9+0Z+5u/a9euynFPGc3Nzft9yWy1jI7Hvb2PgQMH7vO8AzVo0KCMGzeu0Gu8Hb3VT+ADAAAAAMVYv359tm/f/obmHtJ7xFdz5plnVo7Xrl1bOR42bFjl+OWXX646v7m5Odu2bUuSHHnkkZ3G2jO2bduWlpaWqhmNjY2V42oZ+1pD1/GOGb29j47jXdcAAAAAAMCb4x1ZxHd8gekrr7xSOT7mmGMqxy+88ELV+Zs2bUpra2u3OR0/t7a25sUXX6ya0TG/WsbGjRurzu+YMXDgwIwePbry/ahRoypPxfc2o+saAAAAAAB4c7wji/jNmzdXjgcPHlw5Pv744ysvL121alXV+U8//XTluLa2ttNYx8+9yejfv3+OO+64HjMaGhrS0NBQNaM9v+saSqVS5bvVq1dXnf/SSy9V8rtmAAAAAADw5nhHFvEPPfRQ5bhjAT1gwICceuqpSZKHH3646h7tP/jBD5Ls3c6l6x7gEydOzJAhQzqd19WePXvyyCOPJEk+/OEPZ8CAAZ3GzzjjjMrxgw8+2GPG2rVr84tf/CJJ5612umY8//zzWbdu3T7vo1oGAAAAAAAH7pAr4l966aV9jv/4xz/OXXfdlSR573vfm1NOOaXT+EUXXZRk7x7uixcv7jZ/5cqV+dGPfpQkOf/881NT0/l9tzU1NbnggguSJEuXLs3KlSu7ZSxevLiyR3z79To6+eSTK+uqq6tLU1NTp/G2trYsWLAgyd4Xs5599tndMs4999zK9jQLFixIW1tbp/GmpqbU1dUlScaPH++JeAAAAACAghxyRfw555yTz372s/nud7+bZ599Nlu2bMmWLVuyevXqzJ8/PzNnzsyePXtSU1OT//k//2f69On8E5x++umZPHlykmThwoVZuHBhNm7cmHK5nPr6+sydOzetra0ZPXp0Zs2a1eMaLrvssowePTqtra2ZO3du6uvrUy6Xs3Hjxtx0001ZuHBhkmTy5MmVa3V19dVXp6amJuVyORdffHGWL1+exsbGrFu3LvPmzcvjjz+eJLn88sszfPjwbvOHDx+eyy+/PEmybNmyzJs3L+vWrUtjY2OWL1+eiy++OOVyOTU1NfniF7/4hn5rAAAAAAD2r9TW9VHpX3ETJ07s9ALWngwdOjR/+qd/mt/7vd/rcXzbtm2ZNWtW1T3eR44cmUWLFuXEE0+seo21a9dm9uzZKZfLPY5PmDAhdXV1nfao76q+vj7XXnttmpubexyfMWNGrr/++qrzk+S6667LkiVLehzr27dvvva1r+Wcc87ZZ8aBWL9+fbZv355BgwZl3LhxhV3n9Srfdluh+SPnzCk0HwAAAAB4ax1I13nIFfEPPfRQnnrqqaxatSoNDQ1pampKc3Nzhg4dmuOOOy6TJk3KH/zBH2TYsGH7zGlpacmSJUty3333ZcOGDWlubs6YMWMyderUXHrppT0+hd5V+/Y2Dz/8cDZt2pS+ffvm2GOPzfTp0zNjxoxu29r0ZP369bnjjjvy5JNPplwuZ+jQoamtrc2FF17YaS/5fVm6dGnuvvvurFmzJlu3bs3IkSPzoQ99KJdccknh5bgiHgAAAAA4FCjiedtSxAMAAAAAh4ID6ToPuT3iAQAAAADg7UQRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQoJoiQr/0pS+lVCrl85//fEaNGtWrOeVyOf/n//yflEql/Nmf/VkRywIAAAAAgLdcIU/E19fXp76+Ptu2bev1nFdeeaUyDwAAAAAADhW2pgEAAAAAgAK9bYr4lpaWJElNTSG75QAAAAAAwEHxtinin3vuuSTJ0KFDD/JKAAAAAADgzfOmPH7+r//6rz1+/8wzz2TLli37nLtnz578/Oc/T11dXUqlUt73vve9GUvqUWNjY84666w0NTUlSc4999zceOONVc9vaWnJkiVLcv/992fDhg3Zs2dPxowZk2nTpuWSSy7J8OHDe3XNO+64Iz/84Q+zadOm9OvXL8ccc0ymT5+eGTNm9Or/AFi/fn2++c1vZsWKFdm8eXOGDh2a2trazJgxI2eccUav7n3p0qVZsmRJ1qxZk61bt2bEiBE59dRT84lPfCLjxo3rVQYAAAAAAK9fqa2tre1AQ973vvelVCpVPrdHdvxuf9ra2lIqlbJgwYJ85CMfOdAl9egLX/hC7r///srnfRXxr7zySmbOnJlVq1b1OD5y5MgsWrQoJ554YtXrrV27NrNnz065XO5xfMKECamrq8vgwYOrZtTX1+faa69Nc3Nzj+MXXnhhvvKVr1SdnyTXXXddlixZ0uNYv379csMNN+Scc87ZZ8YbtX79+mzfvj2DBg16WxX+5dtuKzR/5Jw5heYDAAAAAG+tA+k637Stadra2ip/evpuf38OP/zwfPrTny6shH/88cdz//335+ijj+7V+VdeeWVWrVqVUqmUOXPm5KGHHsqyZcsyf/78DB48OOVyOZ/61KcqT9d31dTUlDlz5qRcLmfIkCGZP39+li1bloceeihz5sxJqVTK008/nSuvvLLqGlauXJkvf/nLaW5uzgknnJDbb789K1asyD333JNp06YlSe6+++4sWrSoasaiRYsqJfy0adNyzz33ZMWKFbn99ttzwgknZM+ePbnmmmuycuXKXv0uAAAAAAC8Pm/K1jTz58/v9PlLX/pSSqVSPve5z2X06NFV55VKpfTv3z+jRo3KSSedlMMPP/zNWE43r776auWp8WuvvTazZ8/e5/mPPvpoHnvssSTJ5z73ucydO7cydt5552Xs2LH5+Mc/noaGhtTV1eULX/hCt4xFixaloaEhpVIpt956ayZOnFgZu+KKKzJgwIAsXLgwjz32WB577LFMnjy5W8aNN96YlpaWjBgxInfeeWeGDRuWJBk+fHhuvvnmzJw5M8uXL88tt9yS//bf/lu3rXIaGxtzyy23JEkmTZqUm2++ufJ/KUyaNCm1tbX52Mc+ls2bN+frX/96vv3tb+/vpwQAAAAA4HV6U4r4c889t9PnL33pS0n2PoF93HHHvRmXOCB/9Vd/lY0bN+b3f//3c/rpp+/3/LvuuitJMmzYsMycObPb+MSJEzNlypQsXbo03/nOd/L5z3++017vLS0tlVJ7ypQpnUr4djNnzswdd9yRpqam3HXXXd2K+GeeeSarV69OksyaNatSwrcrlUq56qqrsnz58uzcuTP33ntvLr300k7n1NfXZ+fOnUn2PuHfdaugYcOGZdasWbnxxhuzatWqrFmzJrW1tfv9fQAAAAAA6L03bWuaju68885885vfzHve854i4l+XdevW5Zvf/GYGDhyYa665Zr/n79q1KytWrEiSTJ06Nf369evxvLPOOivJ3i1oum7r8tRTT2Xbtm2dzuuqX79+le1lnnjiiezatavT+NKlS7tdq6va2tqMHTs2SfLII490G2/PGDt2bNWCvWN2TxkAAAAAAByYQor4D37wg/ngBz+YAQMGFBHfa62trbn22mvT0tKy321y2j377LPZvXt3kr0vU62m49iaNWs6jXX83JuM3bt357nnnusxY/To0TnqqKOqZowfP77HNXT8rv2cnhx11FGV36WnDAAAAAAADkwhRfzbxZ133plnnnkmtbW1+fjHP96rORs2bKgc7+uJ/jFjxqRPnz7d5nT83KdPn4wZM6ZqRsf8ahn7e7lse8aOHTvS0NBQ+b6hoaGyLU1vM7quAQAAAACAA/em7BG/L01NTXn66aezcePGbN++Pa+99tp+53zmM5854Otu2rQpf/EXf5E+ffrkK1/5Sg477LBezduyZUvl+F3velfV8/r27ZshQ4akqakpTU1NPWYMGTIkffv2rZrR8eWq1TL2tYau401NTZWn23t7Hx3Hu64BAAAAAIADV1gRv3Xr1tx444154IEH0tLS8rrmvhlF/Fe/+tXs3LkzF110UU455ZRez3v11Vcrx/3799/nue3j7U+ed83Y3/yOW/dUy6i2R/3+Mjoe9/Y+duzYsc/zDsT27du77aV/MHzgAx94S6/3drhnAAAAAODgKqSI37FjRz7+8Y/nueeeS1tb2+uaWyqVDvj63//+97N06dKMHDkyV1555QHnAQAAAADAG1VIEf83f/M3efbZZ5Mkxx13XP7oj/4oJ598coYOHVrZV70o27Zty5/92Z8lSa6++uoMHjz4dc0//PDDK8ftL22tpn38iCOO6DFjf/N37dpVOe4po7m5OXv27HlDGR2Pe3sfAwcO3Od5B2LQoEEZN25cYflvV2/1E/gAAAAAQDHWr1+f7du3v6G5hRTx//zP/5xSqZRTTjkld9555363Rnkz3XzzzSmXyznttNPysY997HXPHzZsWOX45Zdfrnpec3Nztm3bliQ58sgje8zYtm1bWlpaUlPT88/c2NhYOe4pY9u2bftcQ9c1dszo7X10HO+6BgAAAAAADlwhRfwLL7yQJJk1a9ZbWsJ3vPby5cv3+wR2fX196uvrkyR//dd/nWnTpuWYY47pltWTTZs2pbW1NUk6zen4ubW1NS+++GJ+/dd/fZ9rrZbx/PPPZ+PGjfu8h/aMgQMHVl7UmiSjRo3KEUcckZ07d/Y6o+saAAAAAAA4cIXsE9O3b98kydFHH11EfKGOP/74yj8erFq1qup5Tz/9dOW4tra201jHz73J6N+/f4477rgeMxoaGtLQ0FA1oz2/6xpKpVLlu9WrV1ed/9JLL1Xyu2YAAAAAAHDgCini258A77j1ylvlS1/6Ur773e/u80+7M844o/Ld7/zO7yRJBgwYkFNPPTVJ8vDDD1fdo/0HP/hBkr3buXTdB3zixIkZMmRIp/O62rNnTx555JEkyYc//OEMGDCg0/gZZ5xROX7wwQd7zFi7dm1+8YtfJEnOPPPMbuPtGc8//3zWrVu3z/uolgEAAAAAwIEppIifPn162traKkXzW+noo4/OiSeeuM8/7Y488sjKdx1f6nrRRRcl2fsPCYsXL+52jZUrV+ZHP/pRkuT888/vtgd8TU1NLrjggiTJ0qVLs3Llym4ZixcvrvxDRfv1Ojr55JNzyimnJEnq6urS1NTUabytrS0LFixIsvfFrGeffXa3jHPPPbfy0tYFCxakra2t03hTU1Pq6uqSJOPHj/dEPAAAAABAAQop4i+66KLU1tbm7//+7/Pkk08WcYlCnX766Zk8eXKSZOHChVm4cGE2btyYcrmc+vr6zJ07N62trRk9enRmzZrVY8Zll12W0aNHp7W1NXPnzk19fX3K5XI2btyYm266KQsXLkySTJ48uXKtrq6++urU1NSkXC7n4osvzvLly9PY2Jh169Zl3rx5efzxx5Mkl19+eYYPH95t/vDhw3P55ZcnSZYtW5Z58+Zl3bp1aWxszPLly3PxxRenXC6npqYmX/ziFw/0ZwMAAAAAoAeltq6PSb9JGhsb85nPfCarV6/OxRdfnOnTp+fYY4/ttgXLwdD+Etdzzz03N954Y4/nbNu2LbNmzaq6x/vIkSOzaNGiTk/Yd7V27drMnj075XK5x/EJEyakrq6u09P4XdXX1+faa69Nc3Nzj+MzZszI9ddfX3V+klx33XVZsmRJj2N9+/bN1772tZxzzjn7zHij1q9fn+3bt2fQoEH7fXnuW6l8222F5o+cM6fQfAAAAADgrXUgXWchRXzHcrqtrS2lUqn3CyqVsnbt2jd7SZ30pohPkpaWlixZsiT33XdfNmzYkObm5owZMyZTp07NpZde2uNT6F21b2/z8MMPZ9OmTenbt2+OPfbYTJ8+PTNmzOi2rU1P1q9fnzvuuCNPPvlkyuVyhg4dmtra2lx44YWd9pLfl6VLl+buu+/OmjVrsnXr1owcOTIf+tCHcskllxRakCviAQAAAIBDwduuiH/f+973hueWSqWqLxblV48iHgAAAAA4FBxI17n/x7HfgHPPPbeIWAAAAAAA+JVTSBE/f/78ImIBAAAAAOBXTp+DvQAAAAAAADiUKeIBAAAAAKBAingAAAAAAChQIXvEb9q06YDmjxkz5k1aCQAAAAAAHFyFFPFnnnlmSqXSG5pbKpWydu3aN3lFAAAAAABwcBRSxCdJW1tbUdEAAAAAAPAro5Ai/jOf+cx+z9m5c2f+4z/+I0888USam5szYcKEnHbaaUUsBwAAAAAADpqDVsS3K5fLufrqq/Pkk0/mvPPOy/nnn1/EkgAAAAAA4KDoc7AXMHLkyNx666059thj89WvfjXr1q072EsCAAAAAIA3zUEv4pOkX79++eM//uM0NzfnjjvuONjLAQAAAACAN83boohPkve9731Jkh//+McHeSUAAAAAAPDmedsU8a2trUmSl19++SCvBAAAAAAA3jxvmyL+scceS5IMHjz4IK8EAAAAAADePG+LIv7ee+/NokWLUiqVMmHChIO9HAAAAAAAeNPUFBH6pS99ab/ntLW1ZevWrVmzZk3K5XLa2trSp0+ffPKTnyxiSQAAAAAAcFAUUsTX19enVCr16ty2tra9C6mpyTXXXJOJEycWsSQAAAAAADgoCinik/9/wV5Nnz59MnDgwBx99NH54Ac/mD/8wz/MMcccU9RyAAAAAADgoCikiP/pT39aRCwAAAAAAPzKeVu8rBUAAAAAAA5VingAAAAAACiQIh4AAAAAAApU2Mta27W1teWRRx7J8uXLs379+jQ1NSVJjjzyyLzvfe/LaaedljPOOCOlUqnopQAAAAAAwFuu0CL+Jz/5Sb70pS/lF7/4ReW7tra2JEmpVMpPfvKT3HXXXRk7dmxuvPHGvP/97y9yOQAAAAAA8JYrrIh/9NFH8+lPfzqvvfZapXwfMGBAhg8fniTZsmVLXn311STJ888/n4svvji33nprfvd3f7eoJQF0s/K26YXmf2DO/YXmAwAAAPD2V0gRv2XLllx11VVpaWlJnz598gd/8Ae58MILc+KJJ1a2oGlra8u6deuyZMmS/MM//ENaWlpy5ZVX5qGHHsqRRx5ZxLIAAAAAAOAtV8jLWr/1rW9l+/btqampyc0335wbbrghJ510Uqd94EulUk466aR89atfzS233JLDDjss27dvz7e+9a0ilgQAAAAAAAdFIUX8o48+mlKplAsuuCBnnnnmfs+fMmVK/vAP/zBtbW159NFHi1gSAAAAAAAcFIUU8Rs3bkyS/N7v/V6v57Sf2/HFrgAAAAAA8KuukCJ+586dSZKhQ4f2es6QIUM6zQUAAAAAgENBIUV8+8tWN2zY0Os5P//5z5Mkw4YNK2BFAAAAAABwcBRSxNfW1qatrS1/93d/1+s53/rWtyovcAUAAAAAgENFIUX8Rz7ykSTJv/3bv+VP/uRP9rndzKuvvpqrr746//Zv/5Yk+ehHP1rEkgAAAAAA4KCoKSJ0+vTp+du//ds888wzeeCBB7JixYp89KMfzYQJEzJy5MgkSblczqpVq/LAAw/k5ZdfTpKccsopmT59ehFLAgAAAACAg6KQIr5UKuW2227LJZdckmeffTabN2/OnXfemTvvvLPbuW1tbUmS448/PrfeemsRywEAAAAAgIOmkK1pkuRd73pX/uEf/iFz5szJkUcemba2th7/DBs2LJdffnn+8R//McOHDy9qOQAAAAAAcFAU8kR8u/79++fzn/98PvOZz2TNmjX52c9+li1btiRJhg0blnHjxuWkk05KTU2hywAAAAAAgIPmLWnAa2pqMn78+IwfP/6tuBwAAAAAALxtFFbEb9++PUly+OGH57DDDtvnua+99lpeffXVJMmgQYOKWhIAAAAAALzlCtkj/l/+5V/y27/92znttNMqW9Hsy5YtW/LhD384H/zgB/P0008XsSQAAAAAADgoCini/+mf/iltbW2ZMmVKRowYsd/zR4wYkTPOOCOtra158MEHi1gSAAAAAAAcFIUU8f/2b/+WUqmUSZMm9XrO5MmTkyRPPfVUEUsCAAAAAICDopAi/he/+EWS5Dd+4zd6PefYY49NkrzwwgtFLAkAAAAAAA6KQor4Xbt2JUmOOOKIXs85/PDDkyQ7duwoYkkAAAAAAHBQFFLEDx48OElSLpd7PWfz5s1JkoEDBxaxJAAAAAAAOCgKKeLHjh2bJFmxYkWv5yxfvjxJ8u53v7uIJQEAAAAAwEFRSBH/oQ99KG1tbfn7v//7/PKXv9zv+S+++GK+/e1vp1Qq5dRTTy1iSQAAAAAAcFAUUsTPmDEjNTU12blzZy699NL89Kc/rXruT3/603zyk5/Mjh07cthhh2XGjBlFLAkAAAAAAA6KmiJCf+3Xfi2f/exnc9NNN+X555/Peeedl1NPPTW/8zu/k1GjRiVJ/vM//zM//vGPs2LFirS1taVUKuXTn/50jj766CKWBAAAAAAAB0UhRXySfOpTn0pTU1MWL16ctra2PPHEE3niiSe6ndfW1pYkmTlzZubOnVvUcgAAAAAA4KAoZGuadl/84hdz++23Z+LEiSmVSmlra+v0p1Qq5YMf/GAWL16cP/mTPylyKQAAAAAAcFAU9kR8u9NOOy2nnXZatm3blrVr16axsTFJMnz48Jx00kkZMmRI0UsAAAAAAICDpvAivt2QIUPyoQ996K26HAAAAAAAvC0UujUNAAAAAAC80yniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKVHOwF/Bm++Uvf5lHHnkk/+///b+sX78+L7/8chobG3PYYYdl9OjRef/7358/+IM/yMSJE/eb1dLSkiVLluT+++/Phg0bsmfPnowZMybTpk3LJZdckuHDh+83o7GxMXfccUd++MMfZtOmTenXr1+OOeaYTJ8+PTNmzEhNzf7/CtavX59vfvObWbFiRTZv3pyhQ4emtrY2M2bMyBlnnNGr32Xp0qVZsmRJ1qxZk61bt2bEiBE59dRT84lPfCLjxo3rVQYAAAAAAK9fqa2tre1gL+LN9K1vfSs33HDDfs87//zzc/311+ewww7rcfyVV17JzJkzs2rVqh7HR44cmUWLFuXEE0+seo21a9dm9uzZKZfLPY5PmDAhdXV1GTx4cNWM+vr6XHvttWlubu5x/MILL8xXvvKVqvOT5LrrrsuSJUt6HOvXr19uuOGGnHPOOfvMeKPWr1+f7du3Z9CgQW+rwr98222F5o+cM6fQfN48K2+bXmj+B+bcX2g+AAAAAG+NA+k6D7mtafr375/TTz89//2///fccccd+f73v58nn3wyDz74YBYsWFApzr/zne/kpptuqppz5ZVXZtWqVSmVSpkzZ04eeuihLFu2LPPnz8/gwYNTLpfzqU99Kk1NTT3Ob2pqypw5c1IulzNkyJDMnz8/y5Yty0MPPZQ5c+akVCrl6aefzpVXXll1DStXrsyXv/zlNDc354QTTsjtt9+eFStW5J577sm0adOSJHfffXcWLVpUNWPRokWVEn7atGm55557smLFitx+++054YQTsmfPnlxzzTVZuXLl/n5aAAAAAADegEPuifj92bNnT/7wD/8wa9euzeGHH54VK1bk8MMP73TOo48+mtmzZydJPv/5z2fu3Lmdxp966ql8/OMfT1tbWy677LJ84Qtf6HadP//zP09dXV1KpVK+9a1vddsK59Zbb83ChQuT7C3LJ0+e3C3j/PPPz+rVqzNixIg88MADGTZsWGWsra0tM2fOzPLly3PEEUfk4Ycf7rZVTmNjY6ZOnZqdO3dm0qRJlfW027JlSz72sY9l8+bNGT9+fL797W/34hd8fTwRz9udJ+IBAAAA6A1PxL8O/fr1y3/9r/81SfLqq6/m3//937udc9dddyVJhg0blpkzZ3YbnzhxYqZMmZJk75P1LS0tncZbWloqpfaUKVN63I9+5syZOfLIIztdr6Nnnnkmq1evTpLMmjWrUwmfJKVSKVdddVWSZOfOnbn33nu7ZdTX12fnzp1J9j7h37GEb7+/WbNmJUlWrVqVNWvWdMsAAAAAAODAvOOK+CSdXpDar1+/TmO7du3KihUrkiRTp07tNt7urLPOSrJ3C5qu27o89dRT2bZtW6fzuurXr19le5knnngiu3bt6jS+dOnSbtfqqra2NmPHjk2SPPLII93G2zPGjh2b2trafd5HtQwAAAAAAA7MO66Ib21tzT/90z8lSYYMGZL3vve9ncafffbZ7N69O8nel6lW03Gs65PkHT/3JmP37t157rnneswYPXp0jjrqqKoZ48eP73ENHb9rP6cnRx11VEaPHl01AwAAAACAA/OOKOLb2tqyefPmLF++PDNnzsy//uu/JknmzZvX7Yn3DRs2VI7f8573VM0cM2ZM+vTp021Ox899+vTJmDFjqmZ0zK+WcfTRR1ed3zFjx44daWhoqHzf0NBQ2Zamtxld1wAAAAAAwIGr2f8pv7rmzZtXefq9o3e9612ZN29eZsyY0W1sy5Ytnc6rpm/fvhkyZEiamprS1NTUY8aQIUPSt2/fqhkdX65aLWNfa+g63tTUVHm6vbf30XG86xreTNu3b++2hc/B8IEPfOAtvd7b4Z7pmf8WAAAAAHirvCOeiO+oX79+ufDCC3PGGWf0OP7qq69Wjvv377/PrPbx9ifPu2bsb/6AAQMqx9Uyqu1Rv7+Mjse9vY8dO3bs8zwAAAAAAF6/Q/qJ+D//8z/P/Pnz09bWVnmp6je+8Y3cfPPN+bu/+7vccsst+a3f+q2Dvcx3hEGDBmXcuHEHexlvubf6qWvevvy3AAAAAPCrbf369dm+ffsbmntIPxHfv3//DBw4MIMGDcp73vOenH322fnHf/zHjB8/Plu2bMnll1+ebdu2dZpz+OGHV47bX9paTfv4EUcc0WPG/ubv2rWrclwtY8+ePW8oo+Nxb+9j4MCB+zwPAAAAAIDX75Au4nsyYMCAXHXVVUn27qP+/e9/v9P4sGHDKscvv/xy1Zzm5uZKiX/kkUf2mLFt27a0tLRUzWhsbKwcV8vY1xq6jnfM6O19dBzvugYAAAAAAA7cO66IT5Lx48dXjtevX99p7Jhjjqkcv/DCC1UzNm3alNbW1m5zOn5ubW3Niy++WDWjY361jI0bN1ad3zFj4MCBlRe1JsmoUaMqT8X3NqPrGgAAAAAAOHDvyCK+41PqpVKp09jxxx9feXnpqlWrqmY8/fTTlePa2tpOYx0/9yajf//+Oe6443rMaGhoSENDQ9WM9vyuayiVSpXvVq9eXXX+Sy+9VMnvmgEAAAAAwIF7RxbxTz31VOV47NixncYGDBiQU089NUny8MMPV92j/Qc/+EGSvdu5dH0J48SJEzNkyJBO53W1Z8+ePPLII0mSD3/4wxkwYECn8TPOOKNy/OCDD/aYsXbt2vziF79Ikpx55pndxtsznn/++axbt26f91EtAwAAAACAA3PIFfH//u//vs/xrVu35n//7/+dJDnssMN6LJ8vuuiiJHv3cF+8eHG38ZUrV+ZHP/pRkuT8889PTU1Np/GamppccMEFSZKlS5dm5cqV3TIWL15c2SO+/XodnXzyyTnllFOSJHV1dWlqauo03tbWlgULFiTZ+2LWs88+u1vGueeeW9meZsGCBWlra+s03tTUlLq6uiR7t+vxRDwAAAAAwJvvkCvip0+fnk9/+tP57ne/m2effTaNjY1pamrKz372s9x55505++yz8+yzzyZJPvnJT3Z7Ij5JTj/99EyePDlJsnDhwixcuDAbN25MuVxOfX195s6dm9bW1owePTqzZs3qcR2XXXZZRo8endbW1sydOzf19fUpl8vZuHFjbrrppixcuDBJMnny5Mq1urr66qtTU1OTcrmciy++OMuXL09jY2PWrVuXefPm5fHHH0+SXH755Rk+fHi3+cOHD8/ll1+eJFm2bFnmzZuXdevWpbGxMcuXL8/FF1+ccrmcmpqafPGLX3x9PzQAAAAAAL1Sauv6mPSvuHHjxu33nMMOOyyzZs3KFVdc0W2P+Hbbtm3LrFmzqu7xPnLkyCxatCgnnnhi1eusXbs2s2fPTrlc7nF8woQJqaury+DBg6tm1NfX59prr01zc3OP4zNmzMj1119fdX6SXHfddVmyZEmPY3379s3Xvva1nHPOOfvMeKPWr1+f7du3Z9CgQb36u3mrlG+7rdD8kXPmFJrPm2flbdMLzf/AnPsLzQcAAADgrXEgXWfN/k/51fJ3f/d3efLJJ/PUU0/lxRdfzMsvv5w9e/Zk0KBBee9735vf/u3fznnnnZdjjjlmnzlDhgzJXXfdlSVLluS+++7Lhg0b0tzcnDFjxmTq1Km59NJLe3wKvaOTTjop9913XxYvXpyHH344mzZtSt++fXPsscdm+vTpmTFjRrdtbbo699xzc9JJJ+WOO+7Ik08+mXK5nKFDh6a2tjYXXnhhp73kq7n++uszZcqU3H333VmzZk22bt2akSNH5kMf+lAuueSSt1VBDgAAAABwqDnknojn7cUT8bzdeSIeAAAAgN44kK7zkNsjHgAAAAAA3k4U8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBag72AgDeyR5b9NHCrzH5su8Vfg0AAAAAqvNEPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIFqDvYCirB79+4sW7Ysjz/+eFavXp2NGzdm586dGTRoUI4//viceeaZueCCCzJo0KB95rS0tGTJkiW5//77s2HDhuzZsydjxozJtGnTcskll2T48OH7XUtjY2PuuOOO/PCHP8ymTZvSr1+/HHPMMZk+fXpmzJiRmpr9/xWsX78+3/zmN7NixYps3rw5Q4cOTW1tbWbMmJEzzjijV7/J0qVLs2TJkqxZsyZbt27NiBEjcuqpp+YTn/hExo0b16sMAAAAAABev1JbW1vbwV7Em+23fuu3smPHjn2ec9RRR+Wv/uqvcsopp/Q4/sorr2TmzJlZtWpVj+MjR47MokWLcuKJJ1a9xtq1azN79uyUy+UexydMmJC6uroMHjy4akZ9fX2uvfbaNDc39zh+4YUX5itf+UrV+Uly3XXXZcmSJT2O9evXLzfccEPOOeecfWa8UevXr8/27dszaNCgt1XhX77ttkLzR86ZU2g+b56Vt00vNP8Dc+7f5/hjiz5a6PWTZPJl3yv8GgAAAACHugPpOg/JrWl27NiRvn375qyzzsqCBQvyz//8z/mXf/mXPPDAA5k9e3Zqamry0ksvZdasWWloaOgx48orr8yqVatSKpUyZ86cPPTQQ1m2bFnmz5+fwYMHp1wu51Of+lSampp6nN/U1JQ5c+akXC5nyJAhmT9/fpYtW5aHHnooc+bMSalUytNPP50rr7yy6n2sXLkyX/7yl9Pc3JwTTjght99+e1asWJF77rkn06ZNS5LcfffdWbRoUdWMRYsWVUr4adOm5Z577smKFSty++2354QTTsiePXtyzTXXZOXKlb38dQEAAAAAeD0OySL+oosuytKlS7Nw4cJ87GMfy6//+q9n6NChOf7443PVVVflxhtvTJJs3bo1t956a7f5jz76aB577LEkyec+97lcccUVGTt2bEaNGpXzzjsvt912W0qlUhoaGlJXV9fjGhYtWpSGhoaUSqXceuutOe+88zJq1KiMHTs2V1xxRT73uc8lSR577LHKtbq68cYb09LSkhEjRuTOO+/MpEmTMnz48NTW1ubmm2/OaaedliS55ZZb0tjY2G1+Y2NjbrnlliTJpEmTcvPNN6e2tjbDhw/PpEmTcuedd2bEiBFpaWnJ17/+9df5KwMAAAAA0BuHZBF/3XXXZeTIkVXHp0+fnhNOOCFJeizB77rrriTJsGHDMnPmzG7jEydOzJQpU5Ik3/nOd9LS0tJpvKWlJd/+9reTJFOmTMnEiRO7ZcycOTNHHnlkp+t19Mwzz2T16tVJklmzZmXYsGGdxkulUq666qokyc6dO3Pvvfd2y6ivr8/OnTuT7H3Cv1QqdRofNmxYZs2alSRZtWpV1qxZ0y0DAAAAAIADc0gW8b1x/PHHJ0n+8z//s9P3u3btyooVK5IkU6dOTb9+/Xqcf9ZZZyXZuwVN121dnnrqqWzbtq3TeV3169evsr3ME088kV27dnUaX7p0abdrdVVbW5uxY8cmSR555JFu4+0ZY8eOTW1t7T7vo1oGAAAAAAAH5h1bxG/evDlJur0o9dlnn83u3buT7H2ZajUdx7o+Sd7xc28ydu/eneeee67HjNGjR+eoo46qmjF+/Pge19Dxu/ZzenLUUUdl9OjRVTMAAAAAADgw78gifvPmzfnJT36SJHn/+9/faWzDhg2V4/e85z1VM8aMGZM+ffp0m9Pxc58+fTJmzJiqGR3zq2UcffTRVed3zNixY0enF882NDRUtqXpbUbXNQAAAAAAcOBqDvYCDoYFCxakubk5SXLhhRd2GtuyZUvl+F3velfVjL59+2bIkCFpampKU1NTjxlDhgxJ3759q2YMHz68clwtY19r6Dre1NRUebq9t/fRcbzrGt5M27dv77aFz8HwgQ984C293tvhnunZwf5v4a2+fk9rAAAAAOCt8Y57Iv6+++7LPffckyQ588wz87u/+7udxl999dXKcf/+/feZ1T7e/uR514z9zR8wYEDluFpGtT3q95fR8bi397Fjx459ngcAAAAAwOv3jnoifvXq1bn22muTJL/2a7+WP/3TPz3IK3rnGDRoUMaNG3ewl/GWOxhPPfP29Hb4b+HtsAYAAACAX1Xr16/P9u3b39Dcd8wT8f/xH/+R2bNnZ9euXTnyyCNTV1fXaWuYdocffnjluP2lrdW0jx9xxBE9Zuxv/q5duyrH1TL27NnzhjI6Hvf2PgYOHLjP8wAAAAAAeP3eEUX8pk2b8slPfjJbtmzJwIEDs2jRohx33HE9njts2LDK8csvv1w1s7m5Odu2bUuSHHnkkT1mbNu2LS0tLVUzGhsbK8fVMva1hq7jHTN6ex8dx7uuAQAAAACAA3fIF/GbN2/OpZdeml/+8pcZMGBAbrvttpxyyilVzz/mmGMqxy+88ELV8zZt2pTW1tZuczp+bm1tzYsvvlg1o2N+tYyNGzdWnd8xY+DAgZUXtSbJqFGjKk/F9zaj6xoAAAAAADhwh3QRv3Xr1lx66aX5+c9/nr59++Yv//Iv88EPfnCfc44//vjKy0tXrVpV9bynn366clxbW9tprOPn3mT079+/2xP67RkNDQ1paGiomtGe33UNpVKp8t3q1aurzn/ppZcq+V0zAAAAAAA4cIdsEb9jx47MmjUrP/vZz9KnT5/8r//1v3L66afvd96AAQNy6qmnJkkefvjhqnu0/+AHP0iydzuXri9AnDhxYoYMGdLpvK727NmTRx55JEny4Q9/OAMGDOg0fsYZZ1SOH3zwwR4z1q5dm1/84hdJkjPPPLPbeHvG888/n3Xr1u3zPqplAAAAAABwYA7JIn7Pnj2ZO3du5Unwr371q/nIRz7S6/kXXXRRkr17uC9evLjb+MqVK/OjH/0oSXL++eenpqam03hNTU0uuOCCJMnSpUuzcuXKbhmLFy+u7BHffr2OTj755MoWOnV1dWlqauo03tbWlgULFiTZ+2LWs88+u1vGueeeW9meZsGCBWlra+s03tTUlLq6uiTJ+PHjPREPAAAAAFCAQ66If+211/L5z38+P/7xj5Mk8+bNy0c+8pHs2LGj6p+uBfXpp5+eyZMnJ0kWLlyYhQsXZuPGjSmXy6mvr8/cuXPT2tqa0aNHZ9asWT2u47LLLsvo0aPT2tqauXPnpr6+PuVyORs3bsxNN92UhQsXJkkmT55cuVZXV199dWpqalIul3PxxRdn+fLlaWxszLp16zJv3rw8/vjjSZLLL788w4cP7zZ/+PDhufzyy5Mky5Yty7x587Ju3bo0NjZm+fLlufjii1Mul1NTU5MvfvGLr//HBgAAAABgv0ptXVvoX3EvvPBCpk6d+rrmPPzww3nPe97T6btt27Zl1qxZVfd4HzlyZBYtWpQTTzyxau7atWsze/bslMvlHscnTJiQurq6DB48uGpGfX19rr322jQ3N/c4PmPGjFx//fVV5yfJddddlyVLlvQ41rdv33zta1/LOeecs8+MN2r9+vXZvn17Bg0alHHjxhVyjTeifNttheaPnDOn0HzePCtvm15o/gfm3L/P8ccWfbTQ6yfJ5Mu+V/g1AAAAAA51B9J11uz/lHemIUOG5K677sqSJUty3333ZcOGDWlubs6YMWMyderUXHrppT0+hd7RSSedlPvuuy+LFy/Oww8/nE2bNqVv37459thjM3369MyYMaPbtjZdnXvuuTnppJNyxx135Mknn0y5XM7QoUNTW1ubCy+8sNNe8tVcf/31mTJlSu6+++6sWbMmW7duzciRI/OhD30ol1xyyduqIAcAAAAAONQcck/E8/biiXje7jwRDwAAAEBvHEjXecjtEQ8AAAAAAG8ningAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAAqkiAcAAAAAgAIp4gEAAAAAoECKeAAAAAAAKJAiHgAAAAAACqSIBwAAAACAAiniAQAAAACgQIp4AAAAAAAokCIeAAAAAAAKpIgHAAAAAIACKeIBAAAAAKBAingAAAAAACiQIh4AAAAAAApUc7AXAAD3LP7/FZp/3qU/KDQfAAAAYF88EQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFEgRDwAAAAAABVLEAwAAAABAgRTxAAAAAABQIEU8AAAAAAAUSBEPAAAAAAAFUsQDAAAAAECBFPEAAAAAAFAgRTwAAAAAABRIEQ8AAAAAAAVSxAMAAAAAQIEU8QAAAAAAUCBFPAAAAAAAFOj/a+/O43Qq/z+Ov4fZzFjG2AlJZkT2fc1WFEpKRSlZI1FUlEoLaUFabClkL9klsm9ZwqDshBiMYRZjm8Xcvz/md873vude5p7lHpNez8ejR+M+51znOudc93Wu87mvc10E4gEAAAAAAAAA8CAC8QAAAAAAAAAAeBCBeAAAAAAAAAAAPIhAPAAAAAAAAAAAHkQgHgAAAAAAAAAADyIQDwAAAAAAAACABxGIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAeRCAeAAAAAAAAAAAPIhAPAAAAAAAAAIAHEYgHAAAAAAAAAMCDCMQDAAAAAAAAAOBBBOIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4gEAAAAAAAAA8CDv250BT7BYLPr777+1f/9+878jR44oMTFRkrR27VrdddddaaaTlJSkefPmadmyZTp58qQSEhJUsmRJtWrVSt26dVNwcHCaaURFRWn69Olas2aNzp07J19fX5UrV07t27fXM888I2/vtC/BkSNH9MMPP2jbtm26dOmSChQooMqVK+uZZ55R8+bN0z4hktavX6958+bpwIEDio2NVeHChdWgQQO98MILCg0NdSsNAAAAAAAAAED63ZGB+PDwcD3yyCOZSiMuLk49evTQvn37bD4/ceKETpw4oYULF2rKlCm67777nKZx8OBB9e7dW5GRkeZnN27c0N69e7V3714tW7ZM3333nfLly+c0jUWLFundd981f0SQpMjISG3YsEEbNmxQ586d9f7777s8luHDh2vevHk2n507d04LFizQsmXL9NFHH6lDhw4u0wAAAAAAAAAAZMwdGYi3Vrx4cVWpUkXR0dHatWuX29sNGjRI+/btk5eXl/r06aMnnnhC/v7+2rJliz7++GNFRkaqT58+Wrp0qYKCguy2j4mJ0UsvvaTIyEjlz59fb731lho3bqybN29qwYIFmjx5svbu3atBgwZpypQpDvOwe/duvfPOO0pKSlJISIiGDBmiSpUq6fz585owYYLWrFmjuXPnqlSpUurVq5fDNKZMmWIG4Vu1aqV+/fqpRIkSOnjwoD799FMdPXpUw4YNU+nSpVWrVi23zw/+/cK/ecnj+yjVf5LH9wEAAAAAAADkdHfkGPFBQUEaP368tmzZoo0bN+qbb75R/fr13d5+48aN2rRpkyRp4MCBeu2111SmTBkVLVpUHTt21KRJk+Tl5aWIiAh99913DtOYMmWKIiIi5OXlpYkTJ6pjx44qWrSoypQpo9dee00DBw6UJG3atMncV2qffPKJkpKSVLhwYc2YMUONGzdWcHCwKleurG+++UaNGjWSJE2YMEFRUVF220dFRWnChAmSpMaNG+ubb75R5cqVFRwcrMaNG2vGjBkqXLiwkpKS9Omnn7p9fgAAAAAAAAAA7rsjA/F58+ZVq1atVKRIkQxtP2fOHElSwYIF1aNHD7vltWvXVrNmzSRJ8+fPV1JSks3ypKQk/fTTT5KkZs2aqXbt2nZp9OjRw+xJb+zP2p9//qn9+/dLknr27KmCBQvaLPfy8tLgwYMlSdevX9eSJUvs0li0aJGuX78uKaWHv5eXl83yggULqmfPnpKkffv26cCBA3ZpAAAAAAAAAAAy544MxGfGzZs3tW3bNklSy5Yt5evr63C9hx9+WFLKEDS7d++2WbZr1y5duXLFZr3UfH191apVK0nS77//rps3b9osX79+vd2+UqtcubLKlCkjSVq3bp3dciONMmXKqHLlyi6Pw1kaAAAAAAAAAIDMIRCfyrFjxxQfHy9Jql69utP1rJel7klu/W930oiPj9fx48cdplGsWDEVL17caRrVqlVzmAfrz4x1HClevLiKFSvmNA0AAAAAAAAAQOYQiE/l5MmT5t933XWX0/VKliypXLly2W1j/e9cuXKpZMmSTtOwTt9ZGqVLl3aZXyONa9euKSIiwvw8IiLCHJbG3TRS5wEAAAAAAAAAkHnetzsDOU10dLT5d6FChZyu5+Pjo/z58ysmJkYxMTEO08ifP798fHycphEcHGz+7SwNV3lIvTwmJsbs3e7ucVgvT52HrHT16lW7IXxuh1q1amXr/nLCMaeW3edA4jxI9ueA65Didl8HAAAAAACA7ECP+FRu3Lhh/u3n5+dyXWO50fM8dRppbe/v72/+7SwNZ2PUp5WG9d/uHse1a9dcrgcAAAAAAAAASD96xCNb5M2bV6Ghobc7G9nudvR6zok4DznjHOSEPNxunAMAAAAAAJBRR44c0dWrVzO0LT3iU8mTJ4/5tzFpqzPG8oCAAIdppLX9zZs3zb+dpZGQkJChNKz/dvc4AgMDXa4HAAAAAAAAAEg/AvGpFCxY0Pz78uXLTtdLTEzUlStXJElBQUEO07hy5YqSkpKcphEVFWX+7SwNV3lIvdw6DXePw3p56jwAAAAAAAAAADKPQHwq5cqVM/8+e/as0/XOnTun5ORku22s/52cnKzw8HCnaVin7yyNM2fOuMyvkUZgYKA5UaskFS1a1OwV724aqfMAAAAAAAAAAMg8AvGpVKhQwZy8dN++fU7X27t3r/l35cqVbZZZ/9udNPz8/HTvvfc6TCMiIkIRERFO0zDST50HLy8v87P9+/c73f7ChQtm+qnTAAAAAAAAAABkHoH4VPz9/dWgQQNJ0tq1a52O0b5y5UpJKcO5pJ78r3bt2sqfP7/NeqklJCRo3bp1kqSGDRvK39/fZnnz5s3Nv3/99VeHaRw8eFD//POPJKlFixZ2y400Tp8+rUOHDrk8DmdpAAAAAAAAAAAyh0C8A126dJGUMob7tGnT7Jbv3r1bGzZskCR16tRJ3t7eNsu9vb311FNPSZLWr1+v3bt326Uxbdo0c4x4Y3/WqlSpoqpVq0qSvvvuO8XExNgst1gsGjNmjKSUiVkfe+wxuzQef/xxc3iaMWPGyGKx2CyPiYnRd999J0mqVq0aPeIBAAAAAAAAwAPu2ED88ePHtXfvXvO/CxcumMsOHTpks8x60lRJeuCBB9S0aVNJ0rhx4zRu3DidOXNGkZGRWrRokfr27avk5GQVK1ZMPXv2dLj/Xr16qVixYkpOTlbfvn21aNEiRUZG6syZM/riiy80btw4SVLTpk3NfaU2dOhQeXt7KzIyUl27dtXWrVsVFRWlQ4cOacCAAdqyZYskqV+/fgoODrbbPjg4WP369ZMkbd68WQMGDNChQ4cUFRWlrVu3qmvXroqMjJS3t7eGDBmSvhMMAAAAAAAAAHCLd9qr/Dt98MEH2rlzp8Nl/fv3t/n3qFGj1LFjR5vPxowZo549e2rfvn2aOHGiJk6caLO8SJEimjx5soKCghzuIygoSJMmTVLv3r0VGRmpoUOH2q1TvXp1jR071ukx1KpVSyNGjNC7776ro0ePqnv37nbrPPPMM+rVq5fTNHr16qWzZ89q3rx5+u233/Tbb7/ZLPfx8dGIESPshtcBAAAAAAAAAGSNOzYQn1n58+fXnDlzNG/ePC1dulQnT55UYmKiSpYsqZYtW+rFF1902AvdWqVKlbR06VJNmzZNa9eu1blz5+Tj46N77rlH7du31zPPPGM3rE1qjz/+uCpVqqTp06dr+/btioyMVIECBVS5cmV17tzZZix5Zz744AM1a9ZMc+fO1YEDBxQbG6siRYqofv366tatm0JDQ9N1bgAAAAAAAAAA7rtjA/EzZ87MdBre3t567rnn9Nxzz2U4jeDgYA0ePFiDBw/OcBqhoaEaNWpUhreXUiZudSdoDwAAAAAAAADIWnfsGPEAAAAAAAAAAOQEBOIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4gEAAAAAAAAA8CAC8QAAAAAAAAAAeBCBeAAAAAAAAAAAPIhAPAAAAAAAAAAAHkQgHgAAAAAAAAAADyIQDwAAAAAAAACABxGIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAeRCAeAAAAAAAAAAAPIhAPAAAAAAAAAIAHEYgHAAAAAAAAAMCDCMQDAAAAAAAAAOBBBOIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4gEAAAAAAAAA8CAC8QAAAAAAAAAAeBCBeAAAAAAAAAAAPIhAPAAAAAAAAAAAHkQgHgAAAAAAAAAADyIQDwAAAAAAAACABxGIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAeRCAeAAAAAAAAAAAPIhAPAAAAAAAAAIAHEYgHAAAAAAAAAMCDCMQDAAAAAAAAAOBBBOIBAAAAAAAAAPAgAvEAAAAAAAAAAHiQ9+3OAID/riPjH/No+qEvL/Fo+gAAwLPa/TzDo+kvf/J5j6YPAAAAGOgRDwAAAAAAAACABxGIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAeRCAeAAAAAAAAAAAPIhAPAAAAAAAAAIAHEYgHAAAAAAAAAMCDCMQDAAAAAAAAAOBBBOIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIO8b3cGAABAzvDl7NYeTX/gs6s8mj4AAAAAADkVPeIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB7EZK0AAAAAAABumLLwokfT79WxqEfTBwDcPvSIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAe5H27M4D/nsiJszy+jyJ9n/P4PgAAAAAAAADAHfSIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8yPt2ZwAAAAD/88SSNh5Nf8FjKz2aPgAAAADAHoF4APiPW/n9Ix5Nv02PFR5NHwAAAAAAIKcjEA8AAADTw0v6eTT9Xx+b4NH0s0rbRZ97NP1fHn/Do+kDAAAAyFkYIx4AAAAAAAAAAA+iRzwAADnAlBmtPZp+r+dXeTR9AAAAAADgHIH4/5D169dr3rx5OnDggGJjY1W4cGE1aNBAL7zwgkJDQ2939gAAAJCDtF0wxaPp//JEL4+mf6do//Mij6a/7MnHPZo+AAAAUhCI/48YPny45s2bZ/PZuXPntGDBAi1btkwfffSROnTocHsyBwBADjH45zYeTX/Mkys9mj4AALizLfj5kkfTf+LJwh5NHwD+ywjE/wdMmTLFDMK3atVK/fr1U4kSJXTw4EF9+umnOnr0qIYNG6bSpUurVq1atzm3AAAAAAAAzv0+I9Kj6Td8vohH0wfw30Qg/g4XFRWlCRMmSJIaN26sb775Rl5eXua/K1eurHbt2unSpUv69NNP9dNPP93O7AIAAAA5Rruf56W9UiYsf/IZj6aPO0unBX96NP35T1Rxufz5hac9uv8ZHct6NH0AAG43AvF3uEWLFun69euSpEGDBplBeEPBggXVs2dPffLJJ9q3b58OHDigypUr346sAgAA4P+1XfiNR9P/pWN/j6YPAJ4wYtF5j6b/zuMlPJo+AOC/LdftzgA8a/369ZKkMmXKOA2wP/zww+bf69aty5Z8AQAAAAAAAMB/BYH4O9yBAwckSdWqVXO6TvHixVWsWDGb9QEAAAAAAAAAWYOhae5gERER5rA0pUuXdrnuXXfdpYiICJ08eTI7sgZJFyaO9Gj6xfsO82j6wJ1k1vTWHt/Hc91WeXwfwJ3ikUXveTT9FY9/6NH0AQAA/gvOf3bGo+mXeNN1LAv4tyEQfweLjo42/y5UqJDLdY3lMTExnswSAABOjfzR8z+IDHuaH0QA4N+mw8+eHT5z8ZMtPJo+AADwrIvjF3s0/aIvd8iSdLwsFoslS1JCjrNnzx517txZkjRixAh16tTJ6bqvv/66li1bJh8fH/31119Zlof9+/crMTFRkpQ3b94sS9ddV69etfl3duch9f5zQh64DjkjD1yHnJEHrkPOyAPXIWfk4XbvnzzkjP2Th5yxf/KQM/ZPHnLG/slDztg/ecgZ+ycPOWP/5CFn7P9258HHx0dVq1ZN1/b0iIdH3bp1y/zb0Rcmu5GH279/8pAz9k8ecsb+yUPO2D95yBn7Jw85Y//kIWfsnzzkjP2Th5yxf/KQM/ZPHnLG/slDztg/ecgZ+7/debCOebqLQPwdLCAgwPw7Pj7e5brG8sDAwCzNg5+fn+Lj45U7d275+flladoAAAAAAAAAkF3i4+N169atDMU5CcTfwQoWLGj+ffnyZZfrGsuDgoKyNA+VKlXK0vQAAAAAAAAA4N8m1+3OADynaNGiZq/4M2dcz2R99uxZSVK5cuU8ni8AAAAAAAAA+C8hEH8H8/LyUuXKlSWlTJrqzIULFxQRESFJ5voAAAAAAAAAgKxBIP4O17x5c0nS6dOndejQIYfrrFy50vy7RYsW2ZIvAAAAAAAAAPivIBB/h3v88cfN4WnGjBkji8ViszwmJkbfffedJKlatWr0iAcAAAAAAACALEYg/g4XHBysfv36SZI2b96sAQMG6NChQ4qKitLWrVvVtWtXRUZGytvbW0OGDLnNuQUAAAAAAACAO4+XJXUXadyRhg8frnnz5jlc5uPjoxEjRqhDhw7ZmykAAAAAAAAA+A8gEP8fsn79es2dO1cHDhxQbGysihQpovr166tbt24KDQ293dkDAAAAAAAAgDsSgXgAAAAAAAAAADyIMeIBAAAAAAAAAPAgAvEAAAAAAAAAAHgQgXgAAAAAAAAAADyIQDwAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4oEssmPHDoWGhio0NFRnz55Nc/2hQ4cqNDRUXbt2zYbcIb3+K9cns8fZokULhYaG6uuvv86yPG3fvl3PPfecqlevrlq1aumll17SsWPHXG6TnJysJ598UqGhoVqzZk2W5SU7uHMNVq5cqRdeeEH16tXTfffdp9DQUD322GOSPHMNcrJ//vlHQ4cOVYsWLXT//feb9e6hQ4dud9ZsLFy40MybcV/4+uuvs7ReOXv2rJn+jh07siTNrHL16lVNmDBBX3/9taZNm6Zbt26Zy4zz0KJFi0ztI6vSSa+cfn/wRJ2wZ88eff311/r666+1devWLEvXlZx+nrNbVpyPhIQETZkyRY8//rhq1Khh1h8jR450O43bURb+bZKTk/XLL7/o008/1fPPP6+HHnpItWrV0v33369GjRqpe/fumjdvnuLj4zO8j/ReB6P8PPfcc9qyZYs++eQTPfPMM6pXr54qV66sOnXqqFOnTvr666916dKldOVl2bJlZl5OnTqVwSNKv4yWRU/cO1y10/DvdDva167aTlnldrWdcpKoqCht2LBBX331lXr27Kl69eqZ90NPXG9P1g/Z0VZKb5zLkBPqxZyQB0e8b3cGAAAwbNiwQf369bNpeK5fv15//PGHZs+erYoVKzrcbv78+frzzz/1wAMPqFWrVtmV3Wwxe/Zsffjhh7c7GznC+fPn1alTJ8XExNzurMCFhIQE9evXz+bHgWPHjunjjz++jblCRv3555/q2bOnrl27Jkny8fHRt99+q4YNG97mnCG93njjDa1cuTLD21MWUoJY33zzjUqVKqV169Y5XOfGjRsaNGiQw2WXLl3SpUuXtHXrVk2bNk2TJ0/W3Xffna48ZOY6/PXXX+rRo4fd51euXNH+/fu1f/9+zZw5U59//rkeeOCBNNObN2+ehg8fbv574cKFmjNnjkqUKJGOI0q/nFQWaachK9B2yjx36mdJGjhwoHbu3JkteXJVPwwdOlSLFi1S3bp1NXPmzGzJT3bJCfViTsiDMwTiAQA5QlJSkoYPH65bt26pbdu2euWVV5SQkKCPPvpIf/zxhz788EPNmTPHbruYmBiNHTtWvr6+euedd25Dzj1r0qRJkqTatWvr3XffValSpZQrVy7lzp37Nucs+82dO1cxMTHy9/fXmDFjVKNGDfn7+0uS8uTJc5tzBynle/zqq6+aD5JFixbVxYsXtWDBAuXPn19Dhw69zTlEepw4cUK9evXStWvX5O/vL19fX125ckUvv/yypk2bpurVq9/uLMJNJ0+eNIPwXbp0Uffu3RUcHCwpJYiZFsqC+7y8vBQSEqImTZqoevXqKlWqlIoVK6akpCSdPXtWCxYs0OLFi3Xq1Cn17NlTy5Ytc/seltnrcOvWLeXKlUuNGzdWmzZtVL16dRUqVEgxMTFavXq1xo8fr9jYWPXv318//vijKlWq5DStFStW6IMPPpAkFSlSRJcuXdK5c+f04osvas6cOWb5ymo5rSzSTkNm0Xa6PQICAlSpUiWVLVtWCxYs8Mg+XNUP77//vkf2mRPkhHoxJ+TBGYamAQDkCLt27dKFCxdUrFgxffrppypXrpxCQ0P15Zdfys/PT7t371Z4eLjddmPHjlVMTIx69uypMmXK3Iace05UVJQuXrwoSXrxxRdVsWJF5cuXT4GBgWYA+r/k8OHDkqRGjRqpVatWKlSokAIDAxUYGKhcuWjS3G4Wi0XDhg3T2rVrJaWU2TVr1phvqUybNk0TJky4nVlEOoSHh6t79+6Kjo5W3rx59f3332vmzJkqXLiwrl+/rt69e+vIkSO3O5twk1F/StJrr72m0qVLm/Wnr6+vy20pC+kTEBCgZcuW6c0339RDDz2kypUrq3DhwipevLhq166tUaNGafDgwZKkM2fO6Ndff3Ur3ay4DsHBwVq5cqWmTJmiJ554QuXLl1dQUJDuvvtu9erVS999951y5cqlhIQEffHFF07T2bhxo958800lJyerQoUKWrhwoT777DN5e3vr5MmT6tmzp65ever+SXNTTiuLtNOQWbSdst9LL72kpUuXateuXZo9e7b69evnkf38V+uHnHDcOSEPrvDUCgDIEY4ePSpJaty4sU3vvEKFCpm9m1I/XP3111+aP3++SpUqpT59+mRbXrPLjRs3zL/z5ct3G3OSMxjnI3/+/Lc5J3Bk5MiRWrx4sSTplVde0dChQ+Xn56evvvpKHTp0kCR9+eWX2r9//+3LJNxy+fJlde/eXRcuXFDBggU1Y8YM1a5dWxUrVtScOXNUqlQpxcbGqkePHvrnn39ud3bhhps3b5p/p6cOpSx4xpNPPmn+feDAgTTXz6rrUKZMGZUtW9bp8tq1a5tD0mzbtk2JiYl26+zevVsDBw5UYmKiqlSpopkzZ6po0aJ69NFH9fXXX8vPz08HDhzQSy+9ZFPuMisnlkXaacgsd9tOs2fPvn2ZvMM0atRIoaGhHu8Z/V+tH3LCceeEPLjC0DTINkePHtXkyZO1Y8cOxcbGqkiRImrSpIl69+6tUqVKKTQ0VJI0atQodezYUTdu3FCdOnWUmJiozz77zG5ShYSEBNWpU0c3b96Ul5eXtm/frqCgIJt1Nm3apF69ekmStmzZoiJFikhKmdxnzpw5CgsLU2RkpLy8vBQcHKyiRYuqTp06euihh1S1alW7Y1i3bp1mzpypAwcOKCEhQXfddZfatGnjcKzF9Prjjz80bdo07du3T1euXFHx4sXVqlUr9enTx+64pJQJZMLDw9W/f3+98sorWrZsmRYsWKCjR48qKipKXbt21bBhwxzuy2KxaP/+/Vq3bp22bdumU6dO6dq1awoMDNQ999yjFi1aqEuXLsqbN6/D7Y8cOaK1a9dq69atOnHihOLi4pQnTx6VKVNGTZo00QsvvODW66jpLRN79uxR586dJUlTpkxR06ZNnaYdGRmpZs2aKSkpSe+9956effbZNPPjSnqvT3q4Wx43btyo3r17S0oZk7NGjRpO0wwPD1fLli1lsVj00Ucf6amnnrotxxkeHq4ePXro5MmTKl68uKZOnary5cs7XNfoOVWoUCG7ZYULF7ZZR0opxx9++KGSk5P19ttvZ+jX7dRj8x0+fFjfffeddu7cqaioKBUsWFCNGjVSv379XPa2X716tWbNmqWDBw8qKSlJd911lx5++GF1795d+/bt0/PPPy9JWrt2re666y6Xefrjjz80cuRIu8lHjTQMqdM6fPiwevToob1795pjpkop5/OJJ55Q7969nX6nU5+HQ4cOafTo0QoLC9P169fN9fLmzat7773XrCO2bt2qAQMGSJJ8fX2VO3du+fv7Kzo6WsWKFdOmTZt0/PhxTZs2TVu3btX58+clyfxOpt6vs/I3depUm/wuWrRIixYtMv/dv39/JSUladKkSSpTpoxWr15td4wTJkzQl19+KUkqX768Tpw4YXfdV65cqcTEROXJk0dt2rRJ87pLzu8LRpl1V2bL4oABAxQTEyMfHx8lJSXJYrHIy8tLuXLlkpeXl5KSkiRJw4cPV5cuXWzq03LlyrkcNzoyMlJNmjSRxWJR3rx5tWPHDnl72zYhv/76a82cOVNeXl56++23bcpr7ty59cknnyhfvnyaOXOmNm3aZLPt4cOHNWXKFP3xxx+6ePGiLBaLihQporlz52revHmaP3++YmNjJaUMoVGlShV9+umn5vZXr15V9+7ddeTIEcXGxiooKEhBQUGyWCyKi4tTTEyMvL29Vbp0aTVr1szte5S7MlpvpCW990hn1q5dq9mzZ+vgwYO6du2aSpYsqTZt2qhXr14O64QrV66oe/fuOnXqlIoVK6Zp06bZ1Ntly5bVnDlz9OKLL+rvv/9Wt27dNHfuXBUrVsxpHs6dO6eJEydq8+bNunTpkoKDg1W3bl316tXLnDTOuk3jSHrvT/Hx8dq2bZvWrVunsLAwnT17VomJiSpQoIAqVaqkRx99VG3btk3zbZrMXocjR45oxowZ2rFjhy5evJhlZdHd82HULdaMPEtyOZauJ8rCmTNnNGnSJG3ZskWXL19WoUKFzLIQEhJiVxYyev7dbSenHlPYuGdt27ZNFy9elL+/v3bt2qUdO3bY1Gvh4eE251FSusb5ta5D03ojwRPXwVX5qVChgtavX6/ExERFR0eraNGi5naHDh1Snz59dOPGDdWtW1cTJ060qUdatGihKVOmqG/fvvrjjz80YMAAjR8/3unwR127dtXOnTv1+OOP65NPPtHWrVv1ww8/6K+//tL169dVpkwZPf3002rbtq15DooUKaJGjRrptdde04ULF5QvXz41btxYX3zxhYYOHer2OTCcPXtWU6ZM0ebNm3Xx4kXlz59ftWvX1ksvveR0aJ6FCxfqrbfesvnMnXba3LlztXPnTkVERCg5OVnFihVTuXLl1Lx5c7Vv3143b97U9OnTtWHDBp09e1ZJSUkqWrSo6tevrxdeeEHff/+907bC5cuX5e/vLz8/PyUmJurGjRsKCgpS9erV9eSTT6pZs2ZZVi9ai4uL09y5c7Vx40b9/fffiouLU3BwsEqWLKk6deqoY8eOKleunN12ycnJWrp0qZYtW6aDBw8qLi5O+fLlU6VKlfTYY4+pffv28vLycrjP9D7Lpq5H8uTJI19fX8XGxiohIcFMt3Dhwho2bJiaN29uvoVr3Qnon3/+0bhx47Rt2zZFRUWZn+fOnVvFixdXhw4d1L17d+XNm9dhHCL1M3RsbKySk5MlpQSHu3TpYpOmddvpww8/NMe7PnLkiM6cOaMpU6Zoy5YtioyMVIECBVSvXj316tVLO3bs0OLFi3Xy5En5+vqqfPnyeuqpp/T444+7fV2tnT59WuvWrdPmzZt19OhRxcTEyNfXVyVLllSDBg30wgsvqFSpUg6viZ+fn/lcsmzZMpUoUULfffedVq1apfPnzytfvnwqUKCAjh8/blMPf/TRRwoLCzMnta5cubKeffZZPfHEE3b5y+r6ObPcqR+s7dy50y6/Rp2YXhl9ls+KOFd66sWIiAitW7dOGzdu1OHDh3Xp0iV5e3urWLFiqlOnjp5//nmFhISk69jTk4fw8HCb5/OAgAB9++23WrdunS5cuKCAgADVrFlTPXr0UK1atdKdj7QQiEe2WLVqlQYPHmzTqyI8PFzz5s3Tr7/+qu+++85umzx58qhKlSras2ePdu7caReI379/v9nLwmKxaNeuXXaTNBrjrJUvX94Mwn///ff67LPP7PZ37tw5nTt3Tnv37tWxY8c0efJkm+WffPKJpk2bZvPZsWPHdOzYMa1atUoDBw5093TYmTdvnj744APzRiyl3OinTp2q5cuX64cfftA999zjcFuLxaLBgwdr+fLlbu9v7dq1evnll+0+j42NVVhYmMLCwvTzzz/r+++/V+nSpW3WOXz4sMOZpuPi4nTgwAEdOHBA8+fP17fffqv777/faR4yUiZq1qype+65R3///bcWLVrkMhC/ZMkSJSUlydfXV+3atXO6njsyc33Skp7y2KhRIxUqVEiXL1/W0qVLXQbily9fLovFIl9fX7Vp08atvGT1cR47dkw9evRQRESE7rnnHk2dOtXl5F2BgYGSpOjoaLtlRmPX+mHv559/1r59+9S0adMsmaB1xYoVGjJkiE2D/OLFi1q0aJHZOEndSJKkDz74wG7s+qNHj+ro0aNatWqV06CSI46uQVqMddesWeNw+aVLlzR58mStW7dOP/74o3menVmxYoXeeOMNM3BrLS4uzqwjfvjhBzM4Ksk8b0YPhIiICH322WeaNWuW2Yg2OPpOuip/7qhXr54mTZqkf/75RxcuXFDx4sVtlltPfnXp0iWb40193W/cuJHmdZdc3xfcefh3JiNl0ZjA1rpOtVgsNhMfS9LkyZP12GOPqWbNmuYYpCdPntSJEyec/ki2cOFCWSwWSSkPBqmD8DNmzNA333yj3Llza8SIEQ6DkV5eXnrnnXeUN29eTZw4UVLKeV63bp1effVVuzISGRmpBx980NyvITExUXv27NEjjzyili1bSkq5d23dutVm28jISJvt4uPjdfjwYTMQMn78eNWpU8fh8aZHRuuNtGTkHunIxx9/rB9++MHms1OnTmnSpEnasGGD5syZY1Mn3LhxQ3369NHhw4dVpkwZTZs2zeGPh8WLF9fs2bPVs2dPHThwQN27d9esWbNUsGBBu3V3795tjudsiIiI0LJly/Tbb7+ZP5C5kpH705gxY+yOXUr5/m/atEmbNm3SsmXL9M033zgNhGb2Onz//fcaPXq0Tb6zoix6sl1i8ERZ2L59u/r27WvzA++FCxe0dOlS/fbbb/rqq69s1s+K70F62slr1qzRoEGDbOojT73Cbj0cTZUqVZyu54nrkFb5qVatmvm5dbvr1KlT6tGjh+Li4tS8eXNz6MDU6tWrp+nTp6tXr17auHGjhgwZotGjR6cZ3J04caLGjRtn89mRI0f04Ycfavz48bp8+bKKFy+u+Ph4swexJPPfv//+u6ZMmaK33347zXNg2Llzp/r166e4uDjzs8uXL2vVqlVav369Jk+enOkJYG/duqXPP/9c06dPt7unnTp1SqdOndL69et1+fJlTZ061SYvUsqPV2fOnNHChQttfhhwdP+5evWqTaeVyMhIrV69WqtXr1a7du0UFBSkWbNm2eUxPfWitW3btum1116za7tHREQoIiJCYWFhOnHihN3QKleuXFG/fv30xx9/2HweFRWlLVu2aMuWLVqwYIHGjx/v8Afj9DzLHjx40K4esT5nqc/DoEGDNHHiRPn6+iohIcEMyE+dOlWff/65wzb6rVu3FB4ervHjx2v58uX6/vvv7eIQzp6hDVu3blW7du1snqEdtZ2klDhIjx49dOXKFfOzyMhILV++XL/88otNObtx44b27NmjPXv2aNu2bXbP9mmJi4vTQw89ZPd5YmKi2eb9+eef9eKLL2r8+PF261k/T/Ts2VO5cuUyO+ZIKW9rGW22xMRErVmzRgMGDLBrvx44cEBvv/22zp49m6m4y50so20DT8a5nGnXrp1N+ZVSrr9RJy5atEgffvihwx9estrZs2c1ZMgQXbhwwfwsPj5ea9eu1bp16+w6GGUFAvHwuOPHj5s3vyJFiuj1119Xw4YNZbFY9Pvvv2v06NF67bXXHG5bt25d7dmzxyaAYti+fbuklN5xiYmJ2r59u9NAfN26dSWlTFQ1ZswYSVKDBg3Uo0cPlS9fXnnz5tWVK1d04sQJbd682a4BNH/+fLNyqlKligYNGqSKFSsqLi5Oy5Yt06RJkzRq1KgMnZ/Tp09rxIgRqly5sl577TXdd999iouL0/LlyzVx4kRdvHhRffv21dKlSx02dn/++WdFRESoY8eO6ty5s0qXLq2oqCibAFlq3t7eatGihVq0aKHy5curaNGiCgwM1MWLF7Vt2zZNmzZNp0+f1qBBgzR//ny77Rs0aKCWLVvqvvvuU5EiRVSgQAFdunRJYWFhmjp1qv7++28NGDBAK1ascPgAk5ky8eSTT+qzzz7T2rVrdeXKFaevVy9cuFCS1KpVKxUoUMDpuUhLZq+PK+ktj97e3nrkkUc0c+ZM/frrr3r77bed9jBatmyZJKlZs2ZuvYKe1ce5Z88evfTSS4qNjVWVKlX07bffptnrz/jV+/fff9etW7fM1wVjY2O1b98+m3ViY2M1ZsyYLJug9fTp0xoyZIiqVaumvn376r777lNCQoJWrVql0aNHKzY2VsOHD9e8efNstps9e7YZhK9Ro4YGDhyo0NBQXb16Vb/88osmTJjgdm8G62swYMAA80cnozeNlPJK+U8//SRfX18FBATIYrHY9MiRpAcffFBdunRR7ty5NWvWLP3222+SZP6gM2jQoDTPQ7ly5RQYGKiHH35Yd911lw4cOKApU6YoMTFRpUqV0o0bN8xgdv78+XXlyhX5+vpqyZIlGjt2rNkj/fvvv9ddd91lPoSfPn1atWrVsvtOplX+EhMTVaZMGRUqVEhhYWFq3769OUGclHIfuHXrls39wHidV0p54Nq7d6+5rlE/Wl/3Ro0amQGAgQMHauLEiU6vu+TefeGuu+7S2bNnzW1eeeWVNH+YyWhZNPj7++vmzZt66KGHlCdPHi1fvtzmYebChQtmOejatatZB82bN8/pW1TWr0MbvegNixcv1scffywfHx+NHTvW4cOatVdffVX79+/X1q1bFRUVpcGDB6tEiRJ6/fXXVbNmTX3wwQdatWqVJJkPkrVq1VL//v1VqFAhLVq0SD/88IMSExNtevH36dNHbdq00Z9//qn33ntPkhQUFKSiRYtq+vTpio+P1/r16/Xll18qNjZWvXv31tKlS9P9QGots9fKmczcI60tWbJEZ86c0VNPPaWnnnpKpUuX1uXLlzVjxgzNmzdPhw8ftqkTEhMTNWDAAO3Zs0chISGaOnWq2YnBkeDgYM2YMUN9+vTRrl271KtXL02fPt0maHL58mX169dP165dU968efXqq6+qZcuW8vPz0549e/T5559ryJAhdsEpaxm9P+XLl09PPfWUGjZsqNKlS6tIkSJmAODXX3/VnDlztHHjRo0bN05vvvlmll+HZcuWmT+0h4SE6LXXXlO1atUyXRbTez4+/PBDvfvuu1q2bJmGDx8uKeUebXAUHPVEWYiIiFD//v11/fp15c+fX6+++qqaN28uPz8/hYWFafTo0eaY45LM+iGz3wN328mxsbF68803VaZMGQ0YMEA1atRQcnKy/vzzT0kpQ7bs2bNHkydP1uTJk1WyZEm74H5awxxcu3ZN586d09KlSzV9+nRJKfcPZx0mPHEd3Ck/xljVFStWVEBAgKSUe0f37t11+fJltW3b1hwP3pmqVatq1qxZ6t69u3755Rfly5fP5r6d2s6dO7V48WK1bdtW3bp1U5kyZRQREaGxY8dqw4YNunz5skqUKKFbt26pQIECev/991W7dm3dunVLK1eu1GeffaaLFy9qypQpaZ4DQ1xcnF555RWVLFlSr7zyiqpXry4vLy9t2bJFI0eO1JUrV/T2229rzZo1dsf66KOPqnXr1jp//rzatm0rSfr2229Vu3Ztcx3j3H3yySeaMWOGeV569uypatWqyd/fX5GRkdqzZ4/mz5+viRMnKiEhQUFBQRo4cKCaNWsmPz8/7d27V2PHjtXx48fN8mh9/+nZs6c+++wznThxQl5eXvLy8lJycrKqVKmiYcOGmb08ly9frvvvvz9T9aK1/fv3q1evXmaP+p49e6p58+YqWrSorl+/rsOHD2vt2rU2Q0RIKff21157zQzCd+rUSV26dFGJEiV0/vx5zZo1SwsWLND27ds1aNAgffvtt3b7dvdZtl+/fjp58qQSExMVHBysGzdu2OWnQIECZn0QGhqqU6dOaciQIebys2fPavr06eabeAEBASpbtqwOHTqk3Llz69atW6pevbri4uJ08eJF8xnauK8ZcQjpf8/Qly5d0qRJk+Tt7a0333xTAQEBLp+hX331VZ0+fVorVqyQJPXt21d+fn765JNP1LBhQ3l5een333/Xe++9Z/6Q2Lx5c7N8h4eHa9q0aVqyZEmab+Y6UrVqVbVu3VpVqlRRkSJFVLBgQcXExOjQoUOaNm2a9u/fr6lTp5qTQFtfk7Vr15rty4iICHl7e2vkyJHmUKSbN2/WO++8o8TERF26dEmvvfaabt26pQceeMCcVHzz5s1avXq1wsLCNHnyZLVt21b33nuvmb+sqJ+zkjv1g8Vi0fvvv69ly5apVq1amjJlik0a7kyibi2jbaWsjHOlp16899571bhxY9WoUUNFixZVoUKFFBcXp+PHj2vWrFnaunWrhg8frsqVK6tixYpunwd382A979zbb7+tq1evatiwYWrZsqX8/f0VFhamzz//XKdOndLIkSN17733ZvpHWRsWwMN69+5tCQkJsVSvXt3y999/2y0/fvy4pWrVqpaQkBBLSEiIZcGCBeay33//3fz83LlzNtt17drVEhISYnn33XctISEhlvbt29ssj4uLs9x3332WkJAQy4oVKywWi8UyY8YMS0hIiKVBgwaW+Ph4t/J/8+ZNS926dS0hISGWRx991HL9+nW7dRYtWmTmMyQkxHLmzJk00x0yZIi5vrN0lyxZYq7z/fff2yxr3ry5ueyzzz5z61jcFRERYalTp44lJCTE8vvvv6dr26tXr1patWplCQkJscyfP9/hOpkpE5cuXbJUrlzZEhISYpk9e7bD9Pfu3Wtuu3nz5nTl35DZ6+OOjJTH/fv3m/tcu3atw3UOHjxorrN69WqX6WVVOfzqq6/MzzZs2GCpVq2aJSQkxNKtWzfL1atX3Tq2xMRES5MmTSwhISGWd955x3Lx4kXLmTNnLL169bKEhIRYnnrqKXPd4cOHW0JCQixffPGFW2k7Y338PXr0sCQmJtqtM3XqVHOd48ePm5/fuHHDUrt2bUtISIilY8eOlps3b9pt+8svv6RZN7i6BmfOnLHZPvU1WL16tc2yCRMm2KX/6aefmstr166dJefB+I6HhIRY5s6da/Od/Oqrr2zy9Oijjzr9Tqa3/LVs2dISEhJiGTJkiMPj6Ny5syUkJMTy1ltv2Xy+c+dOS0hIiKVNmzaWZ5991iZ/xvEa5+mRRx6xO17r626xeOa+kJGy6Kh8pC4HBw4csISGhtosb9SokcViSalPK1asaAkJCbHUrFnTkpSUZLdP62v38MMPuzwGd1mXkSZNmlguXbrk8DwY/3344YdOz0NISIilYcOGFovFYomPj7c0aNDAvNZxcXF25/HAgQOW+++/3xISEmJ5+eWXM5T/zNQb7sjMPdJisW0fOKoTLBaLpU+fPjZlwVM++OADS0hIiKVixYqWP/74w255ZGSkpVGjRmZ+re8lnr4Pb9y40TzPcXFxdsszcx0clcXU0lsWM3s+FixYYC67HYz2+n333WfZu3ev3fLLly9bmjZtaubxwQcfzLLvgat2snV99NBDD1muXLni8jiM9Zs3b+7Wca9bt85hPV2pUiXLoEGDHJYNT0hv+THu75723HPPmft777337JbHx8dbWrRoYZ6zZs2aWaKiouzWGzt2rCUkJMRSuXLlNM+p9TV/7LHHLNeuXbNbZ9WqVeY6GzdudJqW9X14+/btdsvDwsLM5a+88orD+4XFYrH07dvXEhISYqlSpYrl0KFDdstjY2MtDz30kMP2y7Rp08zP5s2bZ3f/SU5Otrz66qvmZ47SN6RVLxqSk5MtDz/8sCUkJMRSr149y8mTJ52um/qYrduuY8aMcbiNddt1zZo1TtN2xvpZ1jgeo41otHsqVqxoWbJkiV09cv78eZv7klH2QkJCLE8//bTl2rVrTuMQ586dM/dr7MeIQ6QlrWdo6zrcUb1o/YwYEhJi6dOnj10aQ4cONZe7W4elJTEx0fLMM89YQkJCLOPGjbNbvn37drv6L3V8wWiTGP+NHz/eLp3o6GhLzZo1LSEhIZbRo0c7zEt662eD9ffYuh2SWWnVD0a9/Nxzz2V4H5lpG3gqzpXWcbvjtddes4SEhFjeeOONDG2fVh6sy6WzNuqlS5fM2ES7du0ylA9nmKwVHmW85iZJzz77rMPx4cqXL28zHpq1GjVqmL8GWveKt+7d2KdPH/n6+uro0aM2r8Xt2rXL7AVYr149STL/HRwc7NbrdpK0fv1685X/wYMHK0+ePHbrdOjQweUwLGlxlu6jjz5qjlVv9PBOrUCBAuYYzVmlaNGiatCggaSU3snpERgYqAcffNDptpktE4UKFVLz5s0lyW7MU4NxrkqUKJElv1xm5vq4kpHyWKVKFfOVMqPXe2pLly6VlNIb1NXwPallxXEuXrxY/fr1040bN9SmTRtNnjw5zaFQDN7e3ho+fLhy586tn376SY0bN1bLli21ceNGBQQEmL1cDxw4oB9//FGlSpXSSy+9JCmlV8Hq1av1wQcf6K233tLUqVMdDnHjyrBhwxz27rIeT9HoiSSljKVnvFL3+uuvO3xT4JFHHnE5hFBqzq6BJPO6W18D6zdWihcvbs4hYO3VV181J6m5cuWKzeugjrhzHqwnQQsPD3f5nTTGL5VcfyfdKX/WQ8o4YvQ6Sv0WlfHvevXq2fRMkv53vKnfoHJ23SXP3xfcLYvWb0MYvVpLlChhUw4qVapk1smGyMhInT9/XoUKFTLvj1evXtWWLVvs9mndGz6rX8uUUnp1OZoXQpI5NuySJUtsevXXrFnTZj3ju7du3TpdvnxZUsp3Mm/evHbnsVKlSnr66afN9VO/UZJe6a030pLZe6S11GXBmvGqr1EWPCEpKcm8H7Vp08amN5KhcOHC6tu3b5ppeeI+3LRpUwUHB+v69esKCwuzWZbZ6+CoLKaWmbLoqXaJpyQlJZm9E9u2bWsz9IkhODhY/fr1M/9t3Gcy+z1ITzt54MCB2TapW8OGDdW5c2enc7d4krPyYz2km5+fX7YMCWDIkyePXn/9dbvPfX19zTetkpKS1K9fP4dDzRg9HxMTE+3m2HHl9ddfN3toWmvVqpX5Rml66vDUjJ7wefPm1ciRIx3eLy5fvqz169dLSnnrzFHvz/z589udH+P+8/PPP0tKGUP76aeftrv/eHl5adiwYeYztbG+I67qRWtbtmzRiRMnJElvvPGG7r77bqfrpj5mo+1apEgR9e/f3+E2r776qtk2cPR2dlqKFi1q01Zo27atdu/ebZOfNm3a6NFHH7WrR4oXL253X0pKSlKuXLnMN0GcxSFKlChhPkMbb/cY7ay0pPUMbc1RvWi0wY2e3xs3brRrN7/55ptuP3O6y9vb2xxyMq18G88yqderUKGC+XfBggXN5ztrQUFBatSokSSZb0rDVnrbBtkR58ooYxin9MaiMsJZG7VQoUJmWTx69Gim7gWpEYiHR+3du9e8CaUOBFhz9iq7v7+/WWlYB1b27t2r+Ph43XvvvSpVqpSqV68ui8ViM86csX6FChXMITHuu+8+SSlDNIwePdqtQJ1x0w4ICDAr//QcQ1rSStc4b8ePH7cbR0uS6tevn+4hUaSUhur8+fPVu3dvNW3aVFWrVlVoaKj5n/HK/6lTp+y2tVgs+vXXX/XKK6+oRYsWqlatms2233//vdNtM1smpJThaaSUVyKNRqAhPj7efG2vQ4cO6ZpsyJHMXh9XMlIepZSbqZTy4G49DqSU0ugzHnZbt27tdmMrK45z2rRpGjp0qJKSkvTMM8/oiy++SHdjr2XLlvruu+9Uq1Yt+fv7KzAwUA888IDmzp2rypUr20zQ+tZbb8nf318JCQnq06eP+vfvrzlz5mjhwoX69NNP9cgjj+jw4cNu7bd06dIOH/SllIafUYdYN2iNV/sDAgJcNrJbtGjhVh7SugZGA8G4BhaLxWZ4gUaNGjl87dLX19d8KJBkN3a2NevzkLqOcPajzqlTp2y+k46CSRs3bpTk/DvpbvlL/TpxasZ1OHv2rM6dO2d+bh2It75WxvFevXrVfHA3lju77pJn7wvpKYsHDx40lxmvMDds2NCuHHTr1s0uLaMcvPjii+ZnqSexio+PN+8F1g9aWcnVfcAYWz0uLk7Hjh0zP//rr78crm9clzx58uiBBx5wmq4xDMStW7dsvkPplZF6Iy1ZcY80OCoLBut8u6oTMuPYsWPm0GqujsXVMilz96eoqChNnDhRXbp0Uf369VW5cmWb9opRX6Vur2T2OniyLHqyXeIpR48eNecIcHVPNOZ9kP43NFVmvwfutpO9vLzS1XnBXU2bNjXHZ/799981c+ZMPfroo9q0aZOeffZZjR07Nsv36Yqz8nPmzBmbcYATEhLSvOdmpWrVqjn9EcR62KYmTZo4XMd6Ymx36zRfX1+n7bdcuXKpbNmyktJXh6dmDKXaqlUrp8cXFhZm1jeu5nVq1qyZWafnzZtX5cqVU2xsrI4fPy4ppd0vOb7/FC5c2GxHbt++PUP1orVt27ZJUrrn4rJuu7Zo0cLpc4Kvr6/ZycNZ3ZjWs6zx44Ykm3NvjA9v1C2O6hFH9U6tWrVUpkwZ7dq1S/Hx8QoICFDnzp2VmJgoi8Wi+vXr2zxDS7ZxCOP4M/oMnVb+jPuOETRNTk42fzAwFCxY0K5Dirs2b96swYMHq3Xr1qpRo4ZNvo1JZE+ePGl3Taw7cfz9998Oj8+6HDRt2tTp87tRF2TmO3mnykjbwNNxrrSEhYVp2LBhatu2rWrWrKmKFSuaZcroSBIZGWkX78hq7rZRM/PMkBpjxMOjrMfGdTVplLMJ4qSU3om7d++2CcRbB1WMdXbu3KkdO3aYFUXq3o3G+q1atdKaNWs0ZcoUTZ06Vffff79q1aql2rVrq0GDBna9Iozxo8qWLetybDFXx+BKWuka581isejcuXN2431nZJy3yMhIde/eXUePHk1z3dTj5V+7dk19+/Z1OG5/WttKWVMmGjdurGLFiikiIkILFiywGcNw9erVunLliry8vBxOGJhemb0+rmSkPEpS+/bt9eWXX+rmzZv67bffbI5zx44dunjxoqT/BezdkdnjXLhwoRn47Nu3r1599VW3951aw4YNnfaaXrBggfbu3asmTZqYN8ZJkyZp48aNqlq1qkaNGqXg4GD98MMPmjRpkgYOHKhffvnF5TimUkrPGVeMHgLGBNHS/+qGu+++2+UPPu5OmJfWNTAmujWuQalSpWyCLK6Owfr75OoHHyON9NYR1t9JI6AdHBysxMRExcXFKT4+3uV30t3ylxbjLarExETt2LFDjz/+uOLj47V37155eXmpXr16CgwMVK5cuZScnGw+HP3xxx+6deuWvLy8bO4Zjq675Nn7QnrKovXkl76+vrp+/brD7a17GhmMY2rcuLE5zv/WrVsVGxtrjuG/evVqc6zRZs2aZXnPzfz586tw4cJOl1eoUMH8MS08PNzsJXjmzBmH6xt10N133+3yO299Pqx/sEmvjNQbacmKe6TBVf6sx55NT/7Sw3r8TVfHUrRoUeXLl89hm0HK+P1p165devnll83eXq6k3ndmr4Mny6In2yWeYl0WnP14JaUEC436yJDZ74G77eSCBQt6pHd67ty5zTcDAwMDVahQIdWtW1eVKlXSJ598osmTJ6t8+fIuJ3HMSo7Kz6VLl9SjRw9FRUXJ29tbSUlJ2V5+3K2vnI2Rb71O6sm/nQkODnY5DnNG6nBrV69eNd+MMTrfOGL9/bAe9zo1Hx8fBQYG6sqVK2a9cv78efNHK+vvg6O833vvvdq2bZuOHz9uNymuI87qZOl/9+Fy5cqlq0PY1atXze93Wt9fo36MjY3V1atXbb6f6WmnSv8rE8Y8OtL/6hZH+XB0X6pYsaIiIyPNNxOuX79uM/G0I9Ztysw+Q1tzVC8a5ahSpUpmj3FH95V77rnH4RuQziQlJWnIkCFuTXgdGxvr1txdro6vZMmSTpcZ33NPtVv+zTLSNvB0nMuVTz/9VFOnTnVr3dTf/6zmqp1RpEgRsy7IzDNDavSIh0dZ35wcBRTdWWYE28PDw80Ho9SBeOP/xudxcXF2vRsNxuQzpUuX1q1bt7Rv3z5NnTpV/fr1U8OGDfXRRx/Z/OpmHIOrPLqzPKPbWS+3DroYnA1j4cqbb76po0ePysfHR926ddP06dO1bt067dy50+y1Y/RuSD1j+ahRo8zz3LFjR3377bdas2aNduzYYW5r/IKZelspa8pE7ty5zdculy5darMf47W8OnXq2PSOyajMXp+0pLc8SikPlbVq1ZL0v2FoDMa/rddxR2aP03rSM0evDGeFK1eu2E3QmpycrNmzZ8vLy0ujR4/Wvffeq+DgYHNSvFOnTpnDC7ji7gQ+FqsJBY2eYml9B90dmieta2D9kHnt2jW76+DqGKzz6OqhwUjDWR1h/BhgnR9jYl3jO3nkyBFzn9bH5Oo7mVX1q7+/vznkgVFPhYWFKSEhweyV5OfnZwbgjXPo6A0qa5ZUE0l68r6QnrJo3VvR+DHI0faO8mEcU+7cudWpUydJKd8n6zrlhx9+MP/2xLA0aZ0f6x5s1uXdWc8YY5200rX+Tmak3jZkpN5IS1bcIw2eyF96WJ/bzHxXMnJ/iouLU//+/RUTE6NChQrp9ddf108//aTNmzdr9+7dZnvFqNNSt1cyex08WRY93S7xBHfPp6Plmf0euNtOzkh7OjO6detm/khgTPqeHVKfsytXrqhHjx46ffq0vL299fLLL5vLsrP8uFtfubOeu3Wap+tI6/Pnqi2YnrrSCMAbPeit74eO9mGdd+NHB4vFkqF60ZqxX3fbuIb0HKur+tGdZ1njrTrpf4F4657Xxv6d5SP157du3dKbb75p/rjSrFkzTZ8+XV988YWklGDlnj17bN5qsI5DZPYZ2lXepP/Vs9Y/njn6Dqe3bTplyhQzCN+qVSuNHz9eq1at0vbt2818v//+++b6qa/JxIkTzWXGW0+uji87J1W9k2SkbeDpOJczS5cuNYPwdevW1dixY7VixQpt27bNLFPWkzQnJSVl6f5Tc/f4s/KeSI94eJR1ob5+/brTV/JcBYZq1KghX19fJSQkaOfOnSpSpIj27dsnLy8v1alTR5JUvXp1+fn56fjx44qKijJfKbZex+Dj46MePXqYjc6wsDDt2rVLGzZsUGRkpGbNmqW9e/fqxx9/lLe3t3kMaf3indbyjG5nvTy9jR1H/vnnH3OsrXfeeUfPPPOMw/UcvY56/fp1LVmyRJLUu3dvDR482OG2rn6lzooyIaUMTzN58mRFRkZq8+bNatasmS5cuGAeW1b0hncnH5m9Puktj4ZHH31Uu3bt0o4dOxQREaFixYopPj5ev/32m6SUXvPG+MruyOxxPv/88zp9+rRWrFihjz/+WJL0wgsvuL1/d4wbN05RUVF66aWXzLEoT506pZiYGJUtW9Z8hdjwwAMPaN++fdq7d6/bw8Okh/HQntar2+7etNO6Btbfq8DAwHSVN+tt0wo2uKojrHv+p24UGd9J6+OwPnZX38msrF/r1q2rXbt2aefOnZJk/t/6Yahw4cK6dOmS2SvLWMfd13U9fV9wl/W1NB7KM5KPzp07m69Dz5w5U127dtWFCxfMsRAz8yqzK2nly7rHlHV5d1b2jc+z+76albLqHpkTWJ/bzHxXMnI9V65cqejoaOXKlUszZsxw2svU2Y86mb0OniyL/8bynfp8upJ6+b/9e+CMl5eXqlSporNnz5o/YGcH63N2/fp19e7dW4cPHzbHv7bu3ZxTys+/lbs/tKWuK13NU2AEMK2HqHFnH5Jsxs7PSL3oKM/pDUyl577g7IcMd59lrdtFRgDeGJbGev/O8pH688OHD9sMTWG8hZuQkCA/Pz/9/fffio+Pt5l3xYhDZMUzdOq8pS4nAQEBiouLs3mjyNF3OL315rx58ySljLPvbCitCxcumH+nvibWb5hl53BX/zUZaRvcrucZo0zVrFlTP/zwg8M3yxMTE7N0n664e/xZeU+kRzw8yvp1UGNMMEdSj/NtLfU48Xv27FFCQoJCQkLMnou+vr6qVq2aLBaLduzYkWbvRkPZsmXVoUMHjRgxQhs2bFDXrl0lpYw/u2HDBklSqVKlJEmnT592+eutq2NwJa10jfPm5eXl8lUtd1mPmW1MbOSIo1f9Tp48aTZg0rutISvKhJQyRpwRWDMmHVm0aJGSk5MVGBhojpOYWdl5fdwpj4aHH35Yvr6+Sk5O1i+//CLJdsz49AxLI2X+OHPnzq3Ro0frkUcekSR9/PHH5gRVWeHQoUOaN2+eSpYsaTOBjzHMiqPvuTHJU3onbXWXdd3gKgh68uRJt9JL6xoYDXvjGuTNm9ccQiQt1mMxphWId7eOSB2It/5OSikPi0Z59PX1dfmddLf8ucMIGIeHh+vMmTN2b1BJ/3u13XhgcfYGlTOevi+4q1ixYubf1g+XqVmPr+5I6dKlVblyZUkpx3Ts2DHNnz/f7En39NNPp+uHPXdduXLF5Tif1vk2zrkkp29WGOucOnXKZe8ZZ+nmBFl1j8wJrM+tq2O5ePGiy9fUM3J/MgKboaGhToNN58+fd7rfzF4HT5bF7G43ZgXrfLga+/jy5ct249r/278HrhjX0RP1qzNG+UlISFC/fv3MCTk/+OADtW3bNkeWn3+rvHnzmm1RVxPIWn/3jfHeHUlKSrJpV0kpwxYa5Set74PRHvXz88tQvWjNuA+fPHnS7aGAJNm0XdPKr3EugoKCbH5wcLedaj1nkaMhTYyy7igf1vcl400C6+db4w1cyT4OYaSbJ08ec52seIa25qheNMqR9fxBjr7D6WlTx8TEmEF24xnPEeux6F0dn7vPREi/jLQNbtfzjPEdbtOmjdPhXbPzB2pX34nIyEizLsjKeyKBeHhU9erVzS/X6tWrna5n9OJ1xgis7Ny502HvRut/u1rHFW9vb73yyivmv40Kxxje4/r169q6dWuGj8GZtNJds2aNpJQbflaM02gdrHFW4e7du9fhGLzubHvhwgXt2rXL6f6zqkxI/5u0df369YqOjjaHpXn44Yez7BWq7L4+Bmfl0ZA/f341a9ZM0v+GozH+f//997s9prYhK44zdTB+5MiRWRKMt1gs+uCDD3Tr1i299dZbNoFko2FtvCZqzQjwZWQyY3fUrFlTUkqPHVfjPa5du9at9NK6BsaEOsY18PLyMvMgOf9OJiQkmJNqucOd77kzxndSsu3J1KRJE5ffSXfLnztDBxhvUUnSpk2b7N6gkv43fNKNGze0atUqp29QOePp+4K7KlWqZP7tqhfV9OnT00zLetLWOXPmaO7cuea/n3766Yxl0A2u7gPGA2m+fPlsxtJ2NoeEcV1u3LihzZs3O0131apVklLqrBo1aqQ7z56UlffI261ChQpmjz1Xx+JqmZSx+5NRj7mqw5YtW+Z0WWavgyfL4u1ql2RGSEiI2ZNs3bp1Ttezvl8awcWc9j0w3k5M7/0xtcTERDMIbj0Zqaddv35dmzZt0sCBA822wZtvvqmnnnpKUs4sP/9mDRo0kJRyXp31NK9Ro4ZZ37gq0xs2bDDLnVG3FihQwLw/GvWJI5cvXzbnkHLVlnJVL1ozJnZMSEgwOwS5w7rtum7dOqe9XhMSEszJVlPXje4+y1o/G1ife6ONaNQtjs65db1jzGVjnUbq4T+NmMOKFSvMoJ319ycrnqGd5S91nowJ7XPlyqXq1avbrBMdHW3GSdxhnW9nnY6uX7+u/fv3m/92p1ORp2RV/ZxdsjK/GWkb3K7nGaNcOStT1h0Ns4OrdoZx3iTZPHdnFoF4eFThwoXVtGlTSdLs2bMd/gp64sQJmwd+R4yb27lz58zXupwF4tevX2/+ypZ6nVOnTrnsufrPP/+YfwcFBUmSmjdvbv49ZswYh69ULV682LzpZYSzdJctW2ZOtpJVQ61Y9/Kynk3ecO3aNX3wwQcOt7XuseFo26SkJL377rsue39lVZmQUmbwLlCggBISEvThhx/q9OnTkrLuXBk8dX0yUh6tGRN7HTp0SLt27TIf9jM64VdWHKejYPzMmTMzlB/DokWLFBYWpsaNG9vN2l6uXDnlzp1bZ86csfuxwiijjiaqzArNmzc3H4TGjh3rsEfyypUrzQdtdzi7BtL/foyxvgbWge8//vjD4XZff/11ut4KSKuOkGzHb7Q+7oceesj84cO6R1WPHj3S3K875c/VxJ4G63Hiv//+eyUmJqpixYo23yHr/BtjV4aEhLg9v0F23BfcYf0miLO65ODBg2kGOqWUa2f8sLVgwQLzAfb+++/3aK/IiRMn2vRas2Yc02OPPWZzzcqVK2fTg8Yog82bNzd7H44ePVpXr161ux8dPnzYvL+0bNnS5Vtzt0NW3iNvN29vb/PtrJUrVzoMMFy6dMlm/Fhn0nt/MuqxkydPmm0DaydOnNCkSZOc7i+z18FRWUwtM2UxO9uNWcHHx8fsJfnLL7/YBG0MMTExNmXB6HGb074HRt0fFRXltL178uTJNMcV//rrrxUZGSlJdm0bT3vzzTfNH0T69etn3qNzavn5NzPmV7l69areeecdhwG3QoUKqXnz5pJSfgh31Cs6Li5Oo0ePNv9t3aZ54oknJEkHDhzQ/PnzHebj448/thlXPiP1orWGDRuaveo///xzm2eW1BwNZSil9DQdP368w22+/vprs0ON8SORISPPssuXLzeDjkZ+Vq5cqaVLl9rVIxcuXLCpi6pUqWK3D+ux5qX/xRysf2i0HjomK56hrTmqF425mowy9sADD9i1mz/77DOXb1CmFhwcbHakcfZMMGrUKJuhPZytlx3cqZ9zEiO/xo9kmZXetsHtep4xvsPOysrkyZOz9Y03Z23UqKgom+dER3VBRhGIh8e98cYb8vHx0fXr19W1a1ctXrxYFy9e1MWLF7Vo0SI9//zz5sOKM9Y9HMPDw5UrVy67novVqlWTv7+/zp8/77R346RJk9SqVSuNGTNGW7du1fnz53XlyhX9888/WrBggdkDOSAgwGwQ+fn5mbOjHz58WM8//7x+//13RUdH659//tE333yjd955x6ZRkB5FixbViRMn1LVrV5t0x48fr7feekuSdPfdd+vZZ5/NUPqpValSxczriBEjNHv2bJ05c0aXL1/W2rVr9cwzz+jw4cMqV66cw7zWrl1bUsq5nDhxok6ePKmoqCht27ZN3bp106ZNm9KcWTsryoSUcm3at28vKaUHgpRyrtIzSWlaPHl9MlIerTVt2tS8eb755ptKTEyUt7e3y1cCs+M4UwfjR4wYkeFgvPHg4ePjo3fffddueUBAgJo1ayaLxaJBgwZp//79Cg8P16hRo/TXX3/Jz89PrVq1ytC+05InTx4NHDhQkrR//3698MIL2r59u6Kjo3XmzBlNnjzZnIjXHY6uQerZ2VNfg5YtW5qB7127dum9997T4cOHFRMTo0OHDumdd96xmezGHa7qCKOBZv0gefDgQf3yyy+KjIxUbGysihYtapNe8eLF0/xOulv+rIdiccV6eBrJ9dtRxjrpGQPd0/eFzFixYoXCwsJ07NgxffbZZ+rUqZNbE835+fmZdYf1a+bWPeWzWv78+XXjxg117txZa9asUVRUlMNxGi9duqSDBw8qJiZGBw4c0ODBg21+eLh06ZLGjRunEydOmN/J48ePq1WrVurYsaOioqJ0/vx5zZ07Vy+88IISEhIUEBCgN954w2PHlhlZdY/MCV5++WUFBQUpOTlZffr00axZs3T+/HlFRUVp9erV6tKli+Lj4132vM3I/emhhx5Srly5lJiYqN69e2vt2rWKjIzUuXPnNGfOHD377LPKkyePwx+5DZm5Dr6+vmbejh8/ri5dumj9+vVZUhazu92YVfr376+8efMqKSlJPXv21Jw5c3ThwgVFRUVp3bp16tKli65du2aWhUaNGuXI74ExjFdCQoK++uorRUREKDExUUlJSea9cfLkyWrfvr2+/fZb7dq1SxEREYqLi9O5c+e0Zs0a9ezZU5MnT5aU8oODJ+vZ1PLkyWMO/9O0aVM9+eSTOnz4sMaNG2eWnzJlyqhDhw66du3av6ZnaU5VrVo1c86kX3/9Vc8++6x+++03RUREKDY2VidOnNDPP/+sS5cuydfXVzdv3tTzzz+vH3/80awrje+HdeDV+sfoLl26KCQkRJL0/vvva+zYsWZP8wsXLujll182J9uUUoK+Ga0XDV5eXho1apR8fHwUFRWlTp06acqUKTpx4oSuXLmiCxcuaOPGjRo+fLiGDBlis23Lli3VuHFjSSk/xrtquz7wwAN28zyl91nWy8tL169f16lTp5QnTx6z/ZCcnKw33njDJqg+Y8YMtW7d2ua+dM8999hNIPrTTz9pwoQJOnTokPbu3Wt2eLAO/lrf17LqGdpQsGBBu3rxxIkTdm8Bp247LVy4MF1tU29vbz344IOSUoaCHTVqlI4dO6bo6Gjt2bNH/fv3108//WTzJnbqa2I9rr6n3/5xp36WZM4paPxnPXTUhQsXbJa5Gi4qq/J75swZzZ49W5cvX1ZSUpKSkpJcdtZzJCNtg9v1PPPwww9LShl2+vXXX9fBgwcVHR2tAwcO6J133tG4cePc/i5khRIlSqhPnz6aOXOmWe+uWbNGnTt3VkREhCSZ5zCrMFkrPO7ee+/V6NGj9frrrysyMtLuZpw/f35NnDhRnTp1cpqGn5+fqlWrZvb4vO++++zGRvb19VX16tW1fft2SSnjgjpqSISHh+vbb791Gpzy9/fX559/bhNM6tSpk44fP67p06dr//79dg3mkJAQDRgwQP3793d+Ipy4++671bdvX3300UcOG+JFixbVxIkTs2x4jdy5c2vkyJHq3bu3rl69qg8//NBmea5cuTRkyBAdPnzYYQ+k999/X88++6xiY2M1btw4jRs3zmZ5t27dlDdvXn3zzTdO85AVZcLw5JNPatasWea/jV4hWcXT1ycj5dHg6+urNm3aaN68eWYwsWHDhhl6MM3q4zSC8VJKYHDEiBHy8vLSc889l658ffnll7p8+bLNBK2pDRkyRLt379bhw4ftyswbb7zh0Qf15557TseOHdOPP/6oPXv22E1Qm7pusJ5wN7W0rkFQUJDdNfDy8lJwcLD5quePP/6oH3/80W7b6tWr24zf6EpadYQkNW7cWDdv3tSuXbuUkJCgQYMGOU3PnWCQu+Vv+PDhbh1D3bp1bXpZuTNMWf369d1K2+DJ+0Jm/P33304nLktL165dtWDBAvPffn5+5gOYJ+TLl0/Dhg3Tq6++qpdfftluea5cuZScnKyVK1dq5cqVNsvy5Mlj/jBksVg0ceJEu57V0dHRio6ONocGMBQoUEDjx493Otb87ZaV98jbrVChQho/frxZn3z00Uf66KOPzOW+vr768ssvNWLECLuxwQ0ZuT/dfffdevXVVzV27FidOnVK/fr1s9kmX758+vrrrzVkyBDFxMQ43G9mr0P79u118eJFjR49WkeOHLGZ38SQkbKY3e3GrFKsWDF9/fXX6tevn2JjY/XBBx/Y9Fr18/PTV199pQ8++EBXrlxRkSJFcuT3oGrVqqpRo4bCwsI0efJkM6Aupdx7jI4Hx44d05gxY1ymVaNGDX3xxRfZOimqdY/HTZs2OZzM/p9//jHrzRkzZqRrqE/YGzJkiCwWi2bMmKGwsDCboSetDRw4UFOnTlV0dLTee+89u+W5c+dWpUqVzInUDb6+vvr222/Vq1cvHTt2zKZM/vzzz+bf7dq1U/ny5fXll19muF60VrVqVX377bd69dVXFRMTo9GjR9v02je0bNnS5t9eXl764osv1K9fP/3xxx9O26716tVz+B1K77Ns+fLldfr0aYfDWEpSbGys+fehQ4fM/Bn3JW9vb9WsWdPmzdP4+Hh9+eWX+vLLL52en9Rt/qx4hjaMGzdO3bt3t6sXpZTza7FYtH79ersex+3bt1fZsmXd2ofhjTfe0K5duxQeHq7p06fbDXfYunVrNW3aVMOGDZMkp88OUkoMx9Hwt1nF3fp5w4YNTgOrP//8s833xnq7rNa8eXOVLl1aZ86c0Ycffmhz3h5//HF98sknbqeV0bbB7Xie6dWrlzZu3KgDBw5o2bJldkNi1axZUy+99JJ69+6dZft0ZeTIkRoyZIhGjBihESNG2Czz8vLS22+/7XRYzIyiRzyyRZs2bbRw4UK1a9dORYoUkY+Pj0qWLKlOnTpp4cKF5mSsrlg3Ap01CK0/d9S78fXXX9dnn32mjh076r777lPhwoXl7e2twMBAVapUST169NCvv/7qsAftW2+9pYkTJ6pBgwbKnz+/8uTJo/Lly6tfv3768ccfMzWOYpcuXTRt2jTzVWZfX1+zh8yyZcvSPd53WurXr6+ffvpJrVu3VsGCBeXj46OiRYuqdevWmjFjhrp16+Z02woVKmjBggXq2LGjeS0LFy6sJk2a2PzimpasKBNSyg3d+DU5d+7cGR6WxRVPXZ/MlEdD6uNN7ySt1rL6OFP3jP/oo49sfjRJy+HDhzVnzhyVKFHCYQDDULZsWf34449q3bq1ChQoIF9fX1WuXFlffPGFOeGtp3h5eenDDz/UV199pbp16ypfvnzKkyeP7r33XrOXiHUvmrQetlNfA2OCKCnltWJH18DoFdWqVSs1btxYwcHB8vb2VnBwsBo3bqzPPvvM4YORK87qCGPokurVq5u9WooVK6Zy5copT548dj2p0vOdzMryZ/0WVe7cuZ2O/W6cOy8vL7OnUnp48r6QXsZ5N8qMl5eXfHx8VLNmTbdfNb/vvvtseiu1adPG48G8li1b6scff9QjjzyiokWLmuNCFylSRL/99pveeustVapUSXny5FG+fPlUo0YNjRw50mYYjwIFCqhOnToKCgoyy2pISIgqVqyo4sWLy8/PTwEBAQoNDdVLL72klStXuj0fwO2SVffInKB27dpavny5nnrqKRUvXty8Rm3bttVPP/3kMBCYWkbqhz59+mj8+PGqW7euAgMD5efnpzJlyqhz585atGiRW2Ugs9ehR48eWrRokZ544gndddddWVYWs7vdmFUaNmyopUuXqmPHjipWrJh8fHxUrFgxtWvXTj/99JOaNWtmvhUTGBiYY78HU6ZMUY8ePXTvvfea90VrAwYM0EcffaR27dopJCTEvC/nzZtX99xzjx599FFNmjTJbOPgzpY7d24NGzZMP//8s5544gmVKVNG/v7+CggIULly5dS6dWt98cUX6t69u1atWqU+ffooNDRUAQEB8vPzU+nSpfXkk09q8eLFTidZLVGihBYuXKh3331XtWvXNu+lAQEBatWqlSZNmqQxY8aoX79+ma4XrTVs2FCrV6/WgAEDVLVqVRUoUEA+Pj4qUaKEatWqpcGDB5sBWmv58+fXjBkz9Omnnzptu/7www82w7tYS8+zbHBwsF09kj9/fgUHB9u0s6WUYOVXX31ld1+yjjGUK1fOpj2fO3duFS5cOM23P7PqGVpKCTgvXLhQTz75pEqWLCkfHx8VKVJE7dq10+LFi522ndL7PCCltMd+/vlnde3aVaVKlZKPj48KFiyounXratSoUfrqq69s3tBIfU2yu72VVv2ck/j7+2v27Nl65plnVLZs2Uy3uTPaNsju55mAgADNmjVLffv21d133y0fHx8VKFBAVatW1dtvv62ZM2dm67UrXbq0+aZdmTJl5Ovrq4IFC6pFixaaPXu2OcRYVvKyuPO+MpANQkNDJaWMM8a4hJDcLxOdO3fWnj179MADD6R7GA7A06ZPn65Ro0YpMDBQu3fvNh+O7mTufieHDh2qRYsWebS3CVK4W5+2a9dOx44dkyTNnz//XxXw/a+5k9pNLVq0UHh4uPr37++0t2hOdSddh9stNjbW7Ejz1VdfqXXr1mluw/kHkFn/hnpk4cKFZrD+yJEjtzk3wL/bjh07zAD72rVrs304UXrEA/hXO3XqlDn2XFYPSwNkBWPipsqVK/8ngvB8J/+9Tp06ZQbhS5QoQRAeQLayHkbBeNsRAADgTkIgHsC/2owZMySlvDbnzuvtQFZzNY7mihUrtGPHDkkpQxz8F/Cd/PeyHsImOycPBPDf4Op+GRUVZY63bD0ZIwAAwJ2EyVoB/OskJSXp5s2bWrNmjTm5T7du3ezG+QOywyOPPKL27durZcuWuueee5Q7d26dPXtWy5cvN8fEL1OmTI561TUhIUGJiYnp3i4gIMBhr353vpMWi8Uc+9d6O0m6deuWrl275nCfPj4+5njvmc2/v7+/zZj9riQlJSk+Pj7d+/Dz83M5KW9qzo7bldy5c2fZ2IkJCQm6ceOGfvrpJy1evFhSykSoriZAzK5zA+DO8sUXX+jcuXPq0KGDOZ50bGystm/frokTJ+rcuXOSUiatBOBZN2/e1K1bt9K9XXZOLmwtq9uu/xW3bt3SzZs3072do/Y3gKzB0xCAf5WzZ8+qZcuWNp9VqFDBI5NoAO6Ii4vT9OnTNX36dIfLixcvrgkTJihPnjzZmzEX3nvvPS1atCjd2zkaQ8/d72R4eLjdeobdu3erZs2aDpc9/vjj+uSTT2w+mzx5sr755pv0Zl8zZsxwOtl3akuXLk3XxFmG9I4v6uy4XcmqMfUdXTtJevfddxUQEOB0u+w6NwDuLBaLRZs2bdKmTZscLvfy8tLQoUPVpEmTbM4Z8N/Tq1cv7dy5M93b3a7xybOy7fpfsmvXrgw9J/8b52wB/i0IxAP41ypatKgaN26sQYMG8Ys9bpvPP/9cmzZt0p9//qlLly4pLi5OgYGBKleunFq0aKEuXboob968tzub2YLv5L9bUFCQXnvtNcb2B+ARXbt2VaFChbR9+3adP39eUVFRyp07t4oWLao6deroueeeU8WKFW93NgEAADzGy2KxWG53JgAAAAAAAAAAuFMxWSsAAAAAAAAAAB5EIB4AAAAAAAAAAA8iEA8AAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4gEAAAAAAAAA8CAC8QAAAAAAAAAAeBCBeAAAAABO7dixQ6GhoQoNDdXChQtvd3YAAACAfyUC8QAAAAAAAAAAeBCBeAAAAAAAAAAAPMjLYrFYbncmAAAAAAAAAAC4U9EjHgAAAAAAAAAADyIQDwAAAAAAAACAB3nf7gwAAAAA/wULFy7UW2+9JUmaMWOG6tatq+XLl2vx4sU6cuSIoqKiVKFCBS1ZssRmu2vXrumnn37Shg0bdOLECcXExCgwMFDlypVTs2bN1KVLF+XPn99mm4SEBDVu3FixsbGqUaOG5s2bl2b+unTpot27dytfvnzaunWr/Pz8JEk7duzQ888/L0kaNWqUOnbs6DSNqKgozZ07V5s3b9bp06cVFxenfPnyqUKFCnrwwQfVqVMn+fv72233xBNP6K+//lLlypW1cOFCu+XXr19X3bp1lZiYKEn69ttv9cADD9it9/nnn+u7775Trly5tH37dhUoUCDN405t06ZNWrRokf78809FRkbq1q1bCgoKUsGCBVWpUiU1atRIrVq1UkBAgMPtk5OTtWrVKv3222/at2+foqKilJSUpMKFCys0NFSNGjVSu3btFBwc7HD7s2fPatasWdq6davOnTunhIQEFSpUSNWrV9fjjz/u8LgN2VnGAAAAkD4E4gEAAIBslpCQoJdeekkbNmxwud62bds0ePBgXb582ebzmJgYhYWFKSwsTD/88IO++uor1alTx1zu6+urhx9+WPPmzVNYWJhOnz6tsmXLOt3PmTNntGfPHknSww8/bAbh02PZsmUaPny4rl27ZvN5VFSUduzYoR07dmjGjBmaMGGCKlSoYLNO/fr19ddff+nQoUOKjY21C6Dv2rXLDMJL0vbt2x0GpLdv3y5Juu+++9IdhE9OTtaQIUO0dOlSu2WRkZGKjIzU0aNHtXjxYs2ePVu1a9e2W+/06dMaMGCADh8+bLfs/PnzOn/+vDZs2KAzZ85o2LBhduvMmzdPI0aMsDlW621//fVXtWzZUmPGjFGePHlcHo+nyxgAAADSh0A8AAAAkM1Gjx6tw4cPq3HjxnriiSdUpkwZxcXF6e+//zbX2bp1q3r37q2kpCQFBQWpc+fOuv/++1W8eHFdvXpV27Zt06xZsxQVFaXevXvrp59+sglwd+jQwewJv3jxYg0cONBpfpYsWSKLxSJJeuyxx9J9PAsWLNDbb78tSSpWrJieffZZhYSEqGjRooqOjtbGjRs1d+5c/fPPP3rxxRe1aNEiFSlSxNy+fv36+u6775ScnKydO3fqwQcftEnfCLAbduzYYZeHuLg4HTp0SJJUr169dB/DvHnzzCB8+fLl9cwzz6hChQoKCgrS9evXdfr0ae3evVvr1q1zuP3Zs2f19NNPKzo6WpJUs2ZNdezYUeXLl5efn58uXryovXv3auXKlQ63X7JkiYYPHy5J8vf31/PPP68mTZrI399fR44c0bRp03TixAmtXbtWr7zyiqZMmSIvLy+nx5MdZQwAAADu87IYLW4AAAAAHmM9bIgk9ezZU2+88YbDda9evaoHH3xQUVFRatCggb755hvlzZvXbr1Tp06pc+fO5nrTp0+3Wd66dWudOnVKd911l9asWeM0cPvQQw/p9OnTKl26tNasWWOzLK2hac6cOaO2bdsqPj5ejz32mEaMGCFfX1+7fYSFhalbt266efOmnnzySY0cOdJcduPGDdWpU0eJiYl67rnn9O6779ps27FjRx04cECtWrXSmjVrHA49s3btWvXr10+S86FrXHn22We1a9culSxZUsuWLXN4vqWUnuaJiYkKDAy0+fyZZ55RWFiYJGngwIFmXlKzWCyKiIhQ8eLFzc9iY2PVokULXb16VQEBAZoxY4aqVKlis93NmzfVo0cP7dq1S5Lja3E7yhgAAADcw2StAAAAQDYrW7asXnvtNafL586dq6ioKOXJk0djx451GhS+++679fLLL0tKGWLkzJkzNsuN3u1nz541A7ipGUPXSCm96NPr+++/V3x8vEqUKKGPPvrIYRBekmrUqKEuXbpIkpYuXaqbN2+ay/LkyaOqVatKsu/9fuXKFbOne/fu3ZU/f34lJyfb9Yo3tvPx8XE4bExaLl26JEmqXLmy0/MtpQz7kzoIv337djMI37JlS6dBeEny8vKyCcJLKQH0q1evSpL69u1rF4SXUnrJf/rpp/Lx8ZEk/fDDDy6PJ7vKGAAAANxDIB4AAADIZo888oi8vZ2PErl69WpJUoMGDZxO6mmoW7eu+bcxzrvhscceM3vBL1682OH2xudeXl4ZGpbG6EHfqlWrNMeWN/KakJCgv/76y2ZZ/fr1JUnHjx83g+KStHPnTiUnJyswMFDVqlUzxylPHbA3/n3//ffbBcrdUaxYMUnSH3/8oVOnTqVrW+vhanr06JHufW/ZskWSlCtXLj311FNO17vrrrvUuHFjSdLhw4ftxnW3ll1lDAAAAO5hjHgAAAAgm1WsWNHpslu3bunAgQOSUgK8oaGhbqcbGRlp8+9SpUqpTp062rlzp1atWqX33nvPJliekJCgX3/9VVLKmOalS5dOz2Ho3Llz5j5nzpypmTNnZjiv9evX1/jx4yWlBNXbtWtn/i1JderUkbe3t+rVq6e1a9faBOKjoqJ07NgxM52M6NSpk3bs2KGYmBi1b99ezZs3V5MmTVStWjWVL19euXPndrqtcb38/f1VrVq1dO/76NGjklJ6nwcFBblct2bNmlq/fr0k6ciRI2rYsKHD9bKrjAEAAMA99IgHAAAAspn12OapxcbGKikpKUPpWg/3YjCGm4mLi7Mb/33Dhg2KjY21WS89XPXITkvqvFavXl3+/v6SbHu7G38bAXbj/ydOnNDFixclpYxjb0x9ldFAfPv27fXGG2/I399fCQkJWrVqld555x21b99e9erV0yuvvKJ169bJ0RRbUVFRkqRChQq57IXuTExMjCSpcOHCaa5rvY6xnSPZWcYAAACQNnrEAwAAANksVy7n/WFu3bpl/t2qVSsNHDjQ7XQLFSpk91nr1q310Ucf6caNG1qyZInatm1rLjOGpfHz89PDDz/s9n4c5bVLly7q3Lmz29umHifd19dXNWvW1O+//24G3y9fvmzX0z0kJESFChXS5cuXtX37dj366KPm+n5+fqpZs2a6j8PQs2dPPf7441qxYoV+//13hYWFKTo6WnFxcfrtt9/022+/qW7dupowYYLy5cuX4f1kh+wsYwAAAEgbgXgAAAAgBwkKCpKXl5csFosSExMVEhKSqfTy5s2rli1bavny5dq6dasuXbqkwoULKzo6Wps2bZKUMsFoRgLLqccWz2xe69evr99//11nzpxReHi49u7dKynlnBhDrXh5ealu3br69ddfzUC8MXFrjRo1nE4W665ChQqpa9eu6tq1q6SUnvcbN27UnDlzdObMGe3cuVMffvihPv/8c3Ob4OBg/f3337p8+bKSkpLS3Ss+KChIFy9etBkb3xnrddIaxsbV/rKyjAEAACBtDE0DAAAA5CA+Pj7mmN379u1TYmJiptM0hp1JSkrS8uXLJUkrVqww087IsDRSyuShRjB4165dmc2mzbAy27dvN3u616tXz5x01nq97du3KyIiQidPnrTbPquUL19e3bt314IFC8wJXVetWmUztMv9998vKWXYln379qV7H8b1PnXqlMvhZiTbyVLTM7a7NU+UMQAAALhGIB4AAADIYR588EFJKWOA//zzz5lOr2HDhipatKik/w1Hs2TJEkkpY443btw4Q+nmypVLLVq0kJQy4ajRwz6j7r//fuXNm1eSbSA+dYDd+Hd4eLjmz59v97knFChQQFWrVpUkxcfH6/r16+ayli1bmn9PnTo13Wkb5z85Odnl9Q4PD9eWLVskSffdd1+mhonJ6jIGAAAA1wjEAwAAADnM888/b/Y0/+STT7R582aX60dFRWnmzJlOl+fOnVvt27eXJB06dEirVq0ye263b99euXPnznBeX3rpJXM4mKFDh+qvv/5yuf758+dtguep81mnTh1J0vr16/XPP/9Isg+w33333SpRooQkafr06ZKkwMBAValSJcPHsWjRIiUkJDhdHhsba56zoKAg5c+f31xWt25d1apVS5K0Zs0aTZw40Wk6FotFFy5csPmsY8eO5g8QEyZM0IEDB+y2i4+P19ChQ83e6y+88IKbR+ZYVpcxAAAAuMYY8QAAAEAOkz9/fn355Zfq2bOnbt68qV69eqlVq1Z68MEHdffdd8vHx0exsbE6evSotm/frs2bNys4ONgc19yRDh066Pvvv5ckvfPOOzafZ0bZsmU1YsQIDRkyRJcvX9Yzzzyjtm3bqlmzZipVqpRy5cql6OhoHTlyRFu2bNHOnTtVrVo1derUyWF69evX1/r16xUXFydJKlasmO655x679erVq6fFixeb69WuXTvdY7NbGzp0qD755BO1aNFCNWvWVLly5RQYGKjY2FgdPnxYc+fO1cWLFyVJzz77rN32n332mZ588klFR0dr3Lhx2rRpkzp27KgKFSrI19dXkZGR2rt3r3799Vc1adJEw4YNM7fNnz+/3nvvPb355pu6du2ann32Wb3wwgtq1KiR8uTJo6NHj2rq1Kk6fvy4JKlJkyaZvm6eKGMAAABwjkA8AAAAkAPVr19fM2fO1ODBgxUeHq7Vq1dr9erVTtdPa7LVkJAQVapUSQcPHtSVK1ckpYwxbkyCmhmPPfaY8ubNq2HDhik6OlqLFy82h8BJb15T936vV6+e0/Ws95EVw9LExMRo4cKFWrhwodN1nnzySfXr18/u87vuukvz5s1T//79dezYMe3Zs8dmPHdrTZo0sfvsscce0/Xr1zVy5EjduHFDkyZN0qRJk+zWa9GihcaOHWszZn5GZXUZAwAAgHME4gEAAIAcqkaNGlq1apWWL1+udevW6cCBA4qKilJSUpLy5s2r0qVLq0qVKmrcuLHD4G5qHTp00MGDB23+nVVatmypBg0aaOHChdq0aZMOHz6s6OhoWSwWFShQQGXLllW1atXUtGlTp8F1KeXHgYIFCyo6OlqS8wC7s3HjM+qXX37R5s2btWfPHp06dUpRUVGKiYmRr6+vSpQooRo1aqhjx47mEDSO3H333VqyZImWL1+u3377TX/99ZeioqIkpYzFHxoaqqZNm6pdu3YOt+/cubMaN26sWbNm6ffff1d4eLgSExNVqFAhVatWTR07dtQDDzyQqeNMLavLGAAAABzzslgsltudCQAAAAAAAAAA7lRM1goAAAAAAAAAgAcRiAcAAAAAAAAAwIMIxAMAAAAAAAAA4EEE4gEAAAAAAAAA8CAC8QAAAAAAAAAAeBCBeAAAAAAAAAAAPIhAPAAAAAAAAAAAHkQgHgAAAAAAAAAADyIQDwAAAAAAAACABxGIBwAAAAAAAADAgwjEAwAAAAAAAADgQQTiAQAAAAAAAADwIALxAAAAAAAAAAB4EIF4AAAAAAAAAAA8iEA8AAAAAAAAAAAeRCAeAAAAAAAAAAAPIhAPAAAAAAAAAIAHEYgHAAAAAAAAAMCDCMQDAAAAAAAAAOBB/weQL4TBQzFkowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 489,
       "width": 753
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.dialogue_acts)\n",
    "plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZM0GKviobjM"
   },
   "source": [
    "That's hugely imbalanced, but it's okay. We're going to convert the dataset into negative, neutral and positive sentiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOssB4CKnAX2"
   },
   "source": [
    "The balance was (mostly) restored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9aHyGuTFgyPO"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "You might already know that Machine Learning models don't work with raw text. You need to convert text to numbers (of some sort). BERT requires even more attention (good one, right?). Here are the requirements: \n",
    "\n",
    "- Add special tokens to separate sentences and do classification\n",
    "- Pass sequences of constant length (introduce padding)\n",
    "- Create array of 0s (pad token) and 1s (real token) called *attention mask*\n",
    "\n",
    "The Transformers library provides (you've guessed it) a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers that do the heavy lifting for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMSr7C-F_sey"
   },
   "source": [
    "> You can use a cased and uncased version of BERT and tokenizer. I've experimented with both. The cased version works better. Intuitively, that makes sense, since \"BAD\" might convey more sentiment than \"bad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiLb-ltM-ZRz"
   },
   "source": [
    "Let's load a pre-trained [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfrSbwTQ-wi_"
   },
   "source": [
    "We'll use this text to understand the tokenization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZMitwrqm2eb"
   },
   "outputs": [],
   "source": [
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yO2qBTVl_KPs"
   },
   "source": [
    "Some basic operations can convert the text to tokens and tokens to unique integers (ids):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "iTFhpHpsoWO7",
    "outputId": "b20afc9d-6481-4d95-8fa9-c398d0d167db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
      " Sentence: When was I last outside? I am stuck at home for 2 weeks.\n",
      "   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
      "Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(token_ids))\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzbbKLR8lZbu"
   },
   "source": [
    "### Special Tokens\n",
    "\n",
    "`[SEP]` - marker for ending of a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EXwz47bQvCbc",
    "outputId": "aa94d74a-326a-41df-80b4-a59ac690ee9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', 102)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mip_eGeXwLFF"
   },
   "source": [
    "`[CLS]` - we must add this token to the start of each sentence, so BERT knows we're doing classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_6K4it5HwE6l",
    "outputId": "73351498-edf2-410d-b444-8d7ba31781e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qi6O-yEY09gl"
   },
   "source": [
    "There is also a special token for padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vx7gD5xf1AFK",
    "outputId": "fa5cf8e7-5cd5-4056-afd4-5a781797515d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PAD]', 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GWCfijM0TWB"
   },
   "source": [
    "BERT understands tokens that were in the training set. Everything else can be encoded using the `[UNK]` (unknown) token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4cmfFsbEKQDT",
    "outputId": "2a7d9f8e-6c61-443b-d0f1-d7206cfbb00a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9ap7jdL0LYU"
   },
   "source": [
    "All of that work can be done using the [`encode_plus()`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vea9edaaxSPO",
    "outputId": "389562b7-89d5-4fb3-e2d7-23a41fc15bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=60,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    "  truncation = True# Return PyTorch tensors\n",
    ")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sS69c8WvdOED"
   },
   "source": [
    "The token ids are now stored in a Tensor and padded to a length of 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YzBmcOla0yQR",
    "outputId": "921d377a-4fd6-4939-e4a4-76ce57d4ed34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
       "        1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itAyVPsNdyc1"
   },
   "source": [
    "The attention mask has the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Wiv5LLiw03Ox",
    "outputId": "8fd1b81a-a5b4-461a-f6f9-53d5ca497562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['attention_mask'][0]))\n",
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1RvhC4jNHHy"
   },
   "source": [
    "We can inverse the tokenization to have a look at the special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'When',\n",
       " 'was',\n",
       " 'I',\n",
       " 'last',\n",
       " 'outside',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'home',\n",
       " 'for',\n",
       " '2',\n",
       " 'weeks',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waKjYxTDuaWt"
   },
   "source": [
    "### Choosing Sequence Length\n",
    "\n",
    "BERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUnE5CT9hbeZ"
   },
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for txt in df.utterances:\n",
    "  tokens = tokenizer.encode(txt,truncation=True)\n",
    "  token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI4goUrHf6da"
   },
   "source": [
    "and plot the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "id": "SzE1j4jxmUtd",
    "outputId": "cf03f40b-88a7-43b0-bc2c-32eb6e16c935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 489,
       "width": 731
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(token_lens))\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 70]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW6ajl30t6du"
   },
   "source": [
    "Most of the reviews seem to contain less than 128 tokens, but we'll be on the safe side and choose a maximum length of 160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 50 #based on the distribution graph above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "We have all building blocks required to create a PyTorch dataset. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "      truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2uwsvCYqDJK"
   },
   "source": [
    "The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it'll be easier to evaluate the predictions from our model. Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-vWzoo81dvO"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xz3ZOQXVPCwh",
    "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((173173, 3), (9621, 3), (9621, 3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "We also need to create a couple of data loaders. Here's a helper function to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=df.content.to_numpy(),\n",
    "    targets=df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resnet in /anaconda/envs/test3/lib/python3.7/site-packages (0.1)\n",
      "Requirement already satisfied: keras>=2.0 in /anaconda/envs/test3/lib/python3.7/site-packages (from resnet) (2.2.0)\n",
      "Requirement already satisfied: h5py in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (5.3.1)\n",
      "Requirement already satisfied: keras-applications==1.0.2 in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (1.18.5)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.1 in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (1.0.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda/envs/test3/lib/python3.7/site-packages (from keras>=2.0->resnet) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!/anaconda/envs/test3/bin/pip3 install resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vODDxMKsPHqI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-987fa682ec54>\", line 3, in <module>\n",
      "    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
      "NameError: name 'df_train' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-987fa682ec54>\", line 3, in <module>\n",
      "    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
      "NameError: name 'df_train' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-987fa682ec54>\", line 3, in <module>\n",
      "    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
      "NameError: name 'df_train' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n"
     ]
    }
   ],
   "source": [
    "#BATCH_SIZE = 16\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Let's have a look at an example batch from our training data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y93ldSN47FeT",
    "outputId": "ee6eaa1a-3f03-4e18-c059-02dbf8b8bc14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-e0a71018e473>\", line 1, in <module>\n",
      "    data = next(iter(train_data_loader))\n",
      "NameError: name 'train_data_loader' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 43\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-e0a71018e473>\", line 1, in <module>\n",
      "    data = next(iter(train_data_loader))\n",
      "NameError: name 'train_data_loader' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-e0a71018e473>\", line 1, in <module>\n",
      "    data = next(iter(train_data_loader))\n",
      "NameError: name 'train_data_loader' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "IdU4YVqb7N8M",
    "outputId": "1f67fe37-6634-484f-caa2-1517e80a29d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-21f6e8b09815>\", line 1, in <module>\n",
      "    print(data['input_ids'].shape)\n",
      "NameError: name 'data' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 44\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-21f6e8b09815>\", line 1, in <module>\n",
      "    print(data['input_ids'].shape)\n",
      "NameError: name 'data' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-21f6e8b09815>\", line 1, in <module>\n",
      "    print(data['input_ids'].shape)\n",
      "NameError: name 'data' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 36, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/__init__.py\", line 23, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/__init__.py\", line 41, in <module>\n",
      "    from tensorflow._api.v1.compat.v1 import feature_column\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/_api/v1/compat/v1/feature_column/__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import input_layer\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py\", line 148, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import backend\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/__init__.py\", line 64, in <module>\n",
      "    from tensorflow.python.keras.applications.resnet import ResNet50\n",
      "  File \"/home/intern2020/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/applications/resnet.py\", line 22, in <module>\n",
      "    from keras_applications import resnet\n",
      "ImportError: cannot import name 'resnet'\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Sentiment Classification with BERT and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "440Nd31VTHER"
   },
   "source": [
    "There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification), [BertForQuestionAnswering](https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering) or something else. \n",
    "\n",
    "But who cares, right? We're *hardcore*! We'll use the basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and build our sentiment classifier on top of it. Let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P41FayISNRI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "And try to use it on the encoding of our sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1aoFxbQSn15"
   },
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids=encoding['input_ids'], \n",
    "  attention_mask=encoding['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLLu8zmqbaHV"
   },
   "source": [
    "The `last_hidden_state` is a sequence of hidden states of the last layer of the model. Obtaining the `pooled_output` is done by applying the [BertPooler](https://github.com/huggingface/transformers/blob/edf0582c0be87b60f94f41c659ea779876efc7be/src/transformers/modeling_bert.py#L426) on `last_hidden_state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mUJHXNpIbcci",
    "outputId": "74906d2c-153b-4f40-e682-b6f501d1ecfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4dAot4zbz8k"
   },
   "source": [
    "We have the hidden state for each of our 32 tokens (the length of our example sequence). But why 768? This is the number of hidden units in the feedforward-networks. We can verify that by checking the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nsxB7Qy7b5YN",
    "outputId": "8aa72ce7-ff62-4075-c36f-4a4f7cda8182"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTKi8-rTd_j4"
   },
   "source": [
    "\n",
    "\n",
    "You can think of the `pooled_output` as a summary of the content, according to BERT. Albeit, you might try and do better. Let's look at the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2jIAtRhaSz9c",
    "outputId": "907e30cd-ad8b-4686-9d50-516f29703ebe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o_NiS3WgOFf"
   },
   "source": [
    "We can use all of this knowledge to create a classifier that uses the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJg8m3NQJahc"
   },
   "source": [
    "Our classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.\n",
    "\n",
    "This should work like any other PyTorch model. Let's create an instance and move it to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0yQnuSFsjDp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCPCFDLlKIQd"
   },
   "source": [
    "We'll move the example batch of our training data to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mz7p__CqdaMO",
    "outputId": "7a933577-8c04-42f3-c3ea-ecb9c1c30a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 160])\n",
      "torch.Size([16, 160])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hr1EgkEtKOIB"
   },
   "source": [
    "To get the predicted probabilities from our trained model, we'll apply the softmax function to the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "2rTCj46Zamry",
    "outputId": "04ecb643-ccda-461f-886f-aefe01f9a248",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4167, 0.1629, 0.4204],\n",
       "        [0.5268, 0.1343, 0.3389],\n",
       "        [0.3120, 0.1510, 0.5370],\n",
       "        [0.5324, 0.1180, 0.3496],\n",
       "        [0.6053, 0.1680, 0.2267],\n",
       "        [0.3334, 0.1828, 0.4838],\n",
       "        [0.4221, 0.1138, 0.4641],\n",
       "        [0.3167, 0.2619, 0.4215],\n",
       "        [0.3325, 0.1465, 0.5211],\n",
       "        [0.2637, 0.1996, 0.5367],\n",
       "        [0.4092, 0.1467, 0.4441],\n",
       "        [0.3765, 0.2127, 0.4107],\n",
       "        [0.5731, 0.0732, 0.3537],\n",
       "        [0.4752, 0.1724, 0.3525],\n",
       "        [0.3837, 0.1831, 0.4332],\n",
       "        [0.3463, 0.2110, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76g7FV85H-T8"
   },
   "source": [
    "To reproduce the training procedure from the BERT paper, we'll use the [AdamW](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw) optimizer provided by Hugging Face. It corrects weight decay, so it's similar to the original paper. We'll also use a linear scheduler with no warmup steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "A8522g7JIu5J"
   },
   "source": [
    "# How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning:\n",
    "\n",
    "- Batch size: 16, 32\n",
    "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "- Number of epochs: 2, 3, 4\n",
    "\n",
    "We're going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.\n",
    "\n",
    "Let's continue with writing a helper function for training our model for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4PniYIte0fr"
   },
   "source": [
    "Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using [clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#clip-grad-norm).\n",
    "\n",
    "Let's write another one that helps us evaluate the model on a given data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Using those two, we can write our training loop. We'll also store the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-46992c3ae043>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     20\u001b[0m     outputs = model(\n\u001b[1;32m     21\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-64dc92491dcb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m     _, pooled_output = self.bert(\n\u001b[1;32m     11\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ming/diaAct-classification/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_tuple)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             \u001b[0mreturn_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    804\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ming/diaAct-classification/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_tuple)\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 )\n\u001b[1;32m    465\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ming/diaAct-classification/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m     ):\n\u001b[1;32m    393\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         )\n\u001b[1;32m    396\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ming/diaAct-classification/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         )\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ming/diaAct-classification/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 153\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \"\"\"\n\u001b[1;32m   1955\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1956\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4r8-5zWsiVur"
   },
   "source": [
    "Note that we're storing the state of the best model, indicated by the highest validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLQf52c7fbzr"
   },
   "source": [
    "Whoo, this took some time! We can look at the training vs validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "id": "-FWG7kBm372V",
    "outputId": "9dd7f8cf-8f36-4280-dfff-bdaa8b9b89f2"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsHqkLAuf8pv"
   },
   "source": [
    "The training accuracy starts to approach 100% after 10 epochs or so. You might try to fine-tune the parameters a bit more, but this will be good enough for us.\n",
    "\n",
    "Don't want to wait? Uncomment the next cell to download my pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoGUH8VZ-pPQ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1V8itWtowCYnb2Bc9KlK9SxGff9WwmogA\n",
    "\n",
    "# model = SentimentClassifier(len(class_names))\n",
    "# model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "So how good is our model on predicting sentiment? Let's start by calculating the accuracy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jS3gJ_qBEljD",
    "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdQ7-ylCj8Gd"
   },
   "source": [
    "The accuracy is about 1% lower on the test set. Our model seems to generalize well.\n",
    "\n",
    "We'll define a helper function to get the predictions from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkbnBTI7kd_y"
   },
   "source": [
    "This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHdPZr60-0c_"
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVwoVij2lC7F"
   },
   "source": [
    "Let's have a look at the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFAekw3mmWUi"
   },
   "source": [
    "Looks like it is really hard to classify neutral (3 stars) reviews. And I can tell you from experience, looking at many reviews, those are hard to classify.\n",
    "\n",
    "We'll continue with the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "6d1qxsc__DTh",
    "outputId": "14b8839c-4e14-430c-b185-46b09bd4231e"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wx0U7oNsnZ3A"
   },
   "source": [
    "This confirms that our model is having difficulty classifying neutral reviews. It mistakes those for negative and positive at a roughly equal frequency.\n",
    "\n",
    "That's a good overview of the performance of our model. But let's have a look at an example from our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iANBiY3sLo-K"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "  'class_names': class_names,\n",
    "  'values': y_pred_probs[idx]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "-8D0rb1yfnv4",
    "outputId": "3c2aa437-9c0d-4421-adf6-9d12e87f4a83"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7hj_IZFnn2X"
   },
   "source": [
    "Now we can look at the confidence of each sentiment of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "id": "qj4d8lZyMkhf",
    "outputId": "3e1e4f5d-3ae0-41bd-8ddc-348e85c13e98"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
    "plt.ylabel('sentiment')\n",
    "plt.xlabel('probability')\n",
    "plt.xlim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "### Predicting on Raw Text\n",
    "\n",
    "Let's use our model to predict the sentiment of some raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEPi7zQRsDhH"
   },
   "outputs": [],
   "source": [
    "review_text = \"I love completing my todos! Best app ever!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaN4RnqMnxYw"
   },
   "source": [
    "We have to use the tokenizer to encode the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zA5Or4D2sLc9"
   },
   "outputs": [],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "  review_text,\n",
    "  max_length=MAX_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "et8xlDrKpH60"
   },
   "source": [
    "Let's get the predictions from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Qr_t3rUksumr",
    "outputId": "4a69d750-c56a-40c1-822a-0b3e7df16b3e"
   },
   "outputs": [],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVhwzq7bpPRl"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Nice job! You learned how to use BERT for sentiment analysis. You built a custom classifier using the Hugging Face library and trained it on our app reviews dataset!\n",
    "\n",
    "- [Read the tutorial](https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
    "- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S)\n",
    "- [Read the `Getting Things Done with Pytorch` book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "- Intuitively understand what BERT is\n",
    "- Preprocess text data for BERT and build PyTorch Dataset (tokenization, attention masks, and padding)\n",
    "- Use Transfer Learning to build Sentiment Classifier using the Transformers library by Hugging Face\n",
    "- Evaluate the model on test data\n",
    "- Predict sentiment on raw text\n",
    "\n",
    "Next, we'll learn how to deploy our trained model behind a REST API and build a simple web app to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf39tauBa2V2"
   },
   "source": [
    "## References\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- [L11 Language Models - Alec Radford (OpenAI)](https://www.youtube.com/watch?v=BnpB3GrpsfM)\n",
    "- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)\n",
    "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
    "- [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
    "- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
